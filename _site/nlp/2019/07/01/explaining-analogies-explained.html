<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      },
      "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>&#39;’Analogies Explained’’ … Explained | Carl Allen: Machine Learning Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="&#39;’Analogies Explained’’ … Explained" />
<meta name="author" content="Carl Allen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post provides a `less maths, more intuition’ overview of Analogies Explained: Towards Understanding Word Embeddings (ICML, 2019, Best Paper Honourable Mention). The outline follows that of the conference presentation. Target audience: general machine learning, NLP, computational linguistics." />
<meta property="og:description" content="This post provides a `less maths, more intuition’ overview of Analogies Explained: Towards Understanding Word Embeddings (ICML, 2019, Best Paper Honourable Mention). The outline follows that of the conference presentation. Target audience: general machine learning, NLP, computational linguistics." />
<link rel="canonical" href="http://localhost:4000/nlp/2019/07/01/explaining-analogies-explained.html" />
<meta property="og:url" content="http://localhost:4000/nlp/2019/07/01/explaining-analogies-explained.html" />
<meta property="og:site_name" content="Carl Allen: Machine Learning Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-01T15:44:00+01:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nlp/2019/07/01/explaining-analogies-explained.html"},"headline":"&#39;’Analogies Explained’’ … Explained","dateModified":"2019-07-01T15:44:00+01:00","datePublished":"2019-07-01T15:44:00+01:00","@type":"BlogPosting","url":"http://localhost:4000/nlp/2019/07/01/explaining-analogies-explained.html","author":{"@type":"Person","name":"Carl Allen"},"description":"This post provides a `less maths, more intuition’ overview of Analogies Explained: Towards Understanding Word Embeddings (ICML, 2019, Best Paper Honourable Mention). The outline follows that of the conference presentation. Target audience: general machine learning, NLP, computational linguistics.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Carl Allen: Machine Learning Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Carl Allen: Machine Learning Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

<!--
        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
-->
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">&#39;&#39;Analogies Explained&#39;&#39; ... Explained</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-07-01T15:44:00+01:00" itemprop="datePublished">Jul 1, 2019
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Carl Allen</span></span> • <a href="https://arxiv.org/abs/1901.09813">[arXiv]</a></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>This post provides a `less maths, more intuition’ overview of <a href="https://arxiv.org/abs/1901.09813">Analogies Explained: Towards Understanding Word Embeddings</a> (ICML, 2019, Best Paper Honourable Mention). The outline follows that of the <a href="https://icml.cc/media/Slides/icml/2019/104(13-11-00)-13-11-00-4883-analogies_expla.pdf">conference presentation</a>. Target audience: general machine learning, NLP, computational linguistics.</p>
</blockquote>

<h2 id="background">Background</h2>

<h3 id="word-embeddings">Word Embeddings</h3>

<p><em>Word embeddings</em> are numerical vector representations of words. Each entry, or dimension, of a word embedding can be thought of as capturing some semantic or syntactic feature of the word, and the full vector can be considered as co-ordinates of the word in a high-dimensional space. Word embeddings can be generated explicitly, e.g. from rows of word co-occurrence statistics (or low-rank approximations of such statistics); or by <em>neural network</em> methods such as <em><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Word2Vec</a></em> (W2V) or <em><a href="https://www.aclweb.org/anthology/D14-1162">Glove</a></em>.</p>

<p>The latter, <em>neural word embeddings</em>, are found to be highly useful in natural language processing (NLP) tasks, such as evaluating word similarity, identifying named entities and assessing positive or negative sentiment in a passage of text (e.g. a customer review).</p>

<h3 id="analogies">Analogies</h3>

<p>An intriguing property of neural word embeddings is that <em>analogies</em> can often be solved simply by adding/subtracting word embeddings. For example, the classic analogy:</p>

<p style="text-align: center"><script type="math/tex">``man</script> is to <script type="math/tex">king</script> as <script type="math/tex">woman</script> is to <script type="math/tex">queen\!"</script></p>

<p>can be <em>solved</em> using word embeddings by finding that closest to <script type="math/tex">\mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman}</script>, which turns out to be <script type="math/tex">\mathbf{w}_{queen}</script>
($\mathbf{w}_{x}$ denotes the embedding of word $x$). Note that words in the question (e.g. <script type="math/tex">man</script>, <script type="math/tex">woman</script> and <script type="math/tex">king</script>) are typically omitted from the search.
This suggests the relationship:</p>

<script type="math/tex; mode=display">\begin{equation}
  \mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman} \approx \mathbf{w}_{queen}\,
  \tag{1}\label{eq:one}
\end{equation}</script>

<p>or, in geometric terms, that <strong><em>word embeddings of analogies approximately form parallelograms:</em></strong></p>
<p style="text-align: center"><img src="/assets/parallelogram2.svg" alt="word embeddings of analogies approximately form a paralellogram" height="170px" />.</p>

<p>Whilst this fits our intuition, the phenomenon is intriguing since word embeddings are not trained to achieve it!
In practice, word embeddings of analogies do not <em>perfectly</em> form parallelograms:</p>

<p style="text-align: center"><img src="/assets/analogy_embeddings.png" alt="word embeddings of analogies not quite a paralellogram" height="250px" /></p>
<p>This shows the (exact) parallelogram formed by <script type="math/tex">\mathbf{w}_{king}, \mathbf{w}_{man}</script>, <script type="math/tex">\mathbf{w}_{woman}</script> and  <script type="math/tex">\mathbf{w}_{king} {\small-} \mathbf{w}_{man} {\small+} \mathbf{w}_{woman}</script> fixed in the <script type="math/tex">xy</script>-plane and a selection of word embeddings shown relative to it. We see that the embedding of <script type="math/tex">queen</script> does not sit at its corner, but is the closest to it. Word embeddings of related words, e.g. <script type="math/tex">prince</script> and <script type="math/tex">lord</script>, lie relatively close by and random unrelated words are further away.</p>

<table class="mbtablestyle">
  <tbody>
    <tr>
      <td>We explain the relationship between word embeddings of analogies \eqref{eq:one}, by explaining the gap (indicated) between <script type="math/tex">\mathbf{w}_{queen}</script> and <script type="math/tex">\mathbf{w}_{king} {\small-} \mathbf{w}_{man} {\small+} \mathbf{w}_{woman}</script>; and why it is small, often smallest, for the word that completes the analogy.</td>
    </tr>
  </tbody>
</table>

<p>To understand why <em>semantic</em> relationships between words give rise to <em>geometric</em> relationships between word embeddings, we first consider what W2V embeddings learn.</p>

<h3 id="word2vec">Word2Vec</h3>

<p><img src="/assets/neural_network.png" alt="W2V architecture" style="float: right; margin-right: 0px; margin-top: -10px;" height="250px" /></p>

<p>W2V (SkipGram with negative sampling) is an algorithm that generates word embeddings by training the weights of a 2-layer “neural network”
to predict <em>context words</em> <script type="math/tex">c_j</script> (i.e. words that fall within a context window of fixed size <script type="math/tex">l</script>) around each word  <script type="math/tex">w_i</script> (referred to as a <em>target word</em>) across a text corpus.</p>

<p>Predicting <script type="math/tex">p(c_j\!\mid\! w_i)</script>, for all <script type="math/tex">c_j</script> in a dictionary of all unique words <script type="math/tex">\mathcal{E}</script>, was initially considered with a softmax function, but instead a sigmoid function and negative sampling were used to reduce computational cost.</p>

<p><a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Levy &amp; Goldberg (2014)</a> showed that, as a result, the weight matrices <script type="math/tex">\mathbf{W}, \mathbf{C}</script> (columns of which are the word embeddings <script type="math/tex">\mathbf{w}_i, \mathbf{c}_j</script>) approximately factorise a matrix of <em>shifted</em> Pointwise Mutual Information (PMI):</p>

<script type="math/tex; mode=display">\begin{equation}
\mathbf{W}^\top\mathbf{C} \approx \textbf{PMI} - \log k\ ,
\end{equation}</script>

<p>where $k$ is the chosen number of negative samples and
<script type="math/tex">\begin{equation}
\textbf{PMI}_{i,j} = \text{PMI}(w_i, c_j) = \log\tfrac{p(w_i,\, c_j)}{p(w_i)p(c_j)}
\end{equation}\,.</script></p>

<p>Dropping the <em>shift</em> term “<script type="math/tex">\log k</script>”, an artefact of the W2V algorithm (that we reconsider in the <a href="https://arxiv.org/abs/1901.09813">paper</a>, Sec 5.5, 6.8), the relationship
<script type="math/tex">\begin{equation}
\mathbf{W}^\top\!\mathbf{C} \!\approx\! \textbf{PMI}
\end{equation}</script>
shows that an embedding <script type="math/tex">\mathbf{w}_i</script> can be considered a <em>low-dimensional projection</em> of a row of the PMI matrix, <script type="math/tex">\text{PMI}_i</script> (a <em>PMI vector</em>).</p>

<hr />
<hr />
<p id="proof"><br /></p>
<h2 id="proving-the-embedding-relationship-of-analogies">Proving the embedding relationship of analogies</h2>

<p>From above, it can be seen that the additive relationship between word embeddings of analogies \eqref{eq:one} follows if
(a) an equivalent relationship exists between PMI vectors, i.e.</p>

<script type="math/tex; mode=display">\begin{equation}
  \text{PMI}_{king} - \text{PMI}_{man} + \text{PMI}_{woman} \approx \text{PMI}_{queen}\ ;
  \tag{2}\label{eq:two}
\end{equation}</script>

<p>and (b) vector addition is sufficiently preserved under the low-rank projection induced by the loss function, as readily achieved by a least squares loss function and as approximated by W2V and Glove.</p>

<table class="mbtablestyle">
  <tbody>
    <tr>
      <td>To prove that relationship \eqref{eq:two} arises between PMI vectors of an analogy, we show that \eqref{eq:two} follows from a particular <strong>paraphrase</strong> relationship, which is, in turn, shown to be equivalent to an analogy.</td>
    </tr>
  </tbody>
</table>

<h3 id="paraphrases">Paraphrases</h3>

<p>When we say a word <script type="math/tex">w_*</script> <em>paraphrases</em> a set of words <script type="math/tex">\mathcal{W}</script>, we mean, intuitively, that they are <em>semantically interchangeable</em> in the text. For example, where <script type="math/tex">king</script> appears, we might instead see  <script type="math/tex">man</script> <u>and</u>  <script type="math/tex">royal</script> close together.
Mathematically, a best choice paraphrase word <script type="math/tex">w_*</script> can be defined as that which maximises the likelihood of the context words observed around <script type="math/tex">\mathcal{W}</script>.
In other words, the distribution of words observed around $w_*$ defined over the dictionary <script type="math/tex">p(\mathcal{E}\!\mid\!w_*)</script>, should be similar to that around <script type="math/tex">\mathcal{W}</script> <script type="math/tex">p(\mathcal{E}\!\mid\!\mathcal{W})</script>, as measured by Kullback-Leibler (KL) divergence. Whilst these distributions are discrete and unordered, we can picture this as:</p>

<p style="text-align: center"><img src="/assets/distributions.png" alt="Paraphrase distributions" height="120px" /></p>

<blockquote>
  <p>Formally, we say <script type="math/tex">w_*</script> <strong>paraphrases</strong> $\mathcal{W}$ if the <strong>paraphrase error</strong> <script type="math/tex">{\rho}^{\mathcal{W}, w_*}\!\in\! \mathbb{R}^{n}</script>
 is (element-wise) small, where:</p>

  <script type="math/tex; mode=display">\begin{equation}
    {\rho}^{\mathcal{W}, w_*}_j = \log \tfrac{p(c_j|w_*)}{p(c_j|\mathcal{W})}\ ,   \quad c_j\!\in\!\mathcal{E}.
\end{equation}</script>
</blockquote>

<p>To see the relevance of paraphrases, we compare the sum of PMI vectors of two words <script type="math/tex">w_1, w_2</script> to that of <strong><u>any word</u></strong> <script type="math/tex">w_*</script> by considering each (<script type="math/tex">j^{th}</script>) component of the difference vector <script type="math/tex">\text{PMI}_* - (\text{PMI}_1 + \text{PMI}_2)</script>:</p>

<p style="text-align: center"><img src="/assets/equation_PMI.png" alt="PMI relationship" height="230px" /></p>

<p>We see that the difference can be written as a <em>paraphrase error</em>, small only if <script type="math/tex">w_*</script> paraphrases <script type="math/tex">\{w_1, w_2\}</script>, and <em>dependence error</em> terms inherent to <script type="math/tex">w_1</script> and <script type="math/tex">w_2</script> that <strong><u>do not depend on $w_*$</u></strong>.
Formally, we have:</p>

<blockquote>
  <p>Lemma 1:
For any word <script type="math/tex">w_*\!\in\!\mathcal{E}</script> and word set <script type="math/tex">\mathcal{W}\!\subseteq\!\mathcal{E}</script>, <script type="math/tex">% <![CDATA[
|\mathcal{W}|\!<\!l %]]></script>, where <script type="math/tex">l</script> is the context window size:</p>

  <script type="math/tex; mode=display">\begin{equation}
    \text{PMI}_*
    \ =\
    \sum_{w_i\in\mathcal{W}} \text{PMI}_{i}
     \,+\, \rho^{\mathcal{W}, w_*}
     \,+\, \sigma^{\mathcal{W}}
     \,-\, \tau^\mathcal{W}\mathbf{1}\ .
\end{equation}</script>
</blockquote>

<p>This connects paraphrasing to PMI vector <em>addition</em>, as appears in \eqref{eq:two}. To develop this further, paraphrasing is generalised, replacing <script type="math/tex">w_*</script> by <script type="math/tex">\mathcal{W}_*</script>, to a relationship between any <em>two word sets</em>.
The underlying principle remains the same: word sets paraphrase one another if the distributions of context words around them are similar (indeed, the original paraphrase definition is recovered if <script type="math/tex">\mathcal{W}_*</script> contains a single word). Analogously to above, we find:</p>

<blockquote>
  <p>Lemma 2:
For any word sets <script type="math/tex">\mathcal{W}, \mathcal{W}_*\!\subseteq\!\mathcal{E}</script>; <script type="math/tex">% <![CDATA[
|\mathcal{W}|, |\mathcal{W}_*|\!<\!l %]]></script>:</p>

  <script type="math/tex; mode=display">\begin{equation}
    \sum_{w_i\in\mathcal{W}_*} \text{PMI}_{i}
    \ =\
    \sum_{w_i\in\mathcal{W}} \text{PMI}_{i}
     \,+\, \rho^{\mathcal{W}, \mathcal{W}_*}
     \,+\, (\sigma^{\mathcal{W}} - \sigma^{\mathcal{W}_*})
     \,-\, (\tau^\mathcal{W} - \tau^{\mathcal{W}_*})\mathbf{1}
\end{equation}</script>
</blockquote>

<p>We can apply this to our example by setting <script type="math/tex">\mathcal{W} \!=\! \{woman, king\}</script> and <script type="math/tex">\mathcal{W}_* \!=\!  \{man, queen\}</script>, whereby</p>

<script type="math/tex; mode=display">\begin{equation}
  \text{PMI}_{king} - \text{PMI}_{man} + \text{PMI}_{woman} \ \approx\ \text{PMI}_{queen}\ ,
\end{equation}</script>

<p>if <script type="math/tex">\mathcal{W}</script> paraphrases <script type="math/tex">\mathcal{W}_*</script> (meaning <script type="math/tex">\rho^{\mathcal{W}, \mathcal{W}_*}</script> is small), subject to <em>net</em> statistical dependencies of words within <script type="math/tex">\mathcal{W}</script> and <script type="math/tex">\mathcal{W}_*</script>, i.e. <script type="math/tex">\sigma^{\mathcal{W}} - \sigma^{\mathcal{W}_*}</script> and <script type="math/tex">\tau^\mathcal{W} - \tau^{\mathcal{W}_*}</script>.</p>

<table class="mbtablestyle">
  <tbody>
    <tr>
      <td>Thus, \eqref{eq:two} holds, subject to dependence error, if <script type="math/tex">\{woman, king\}</script> paraphrases <script type="math/tex">\{man, queen\}</script>.</td>
    </tr>
  </tbody>
</table>

<p>This establishes a semantic relationship as a sufficient condition for the geometric relationship we aim to explain – but that semantic relationship is not an analogy.
What remains then, is to explain why a general analogy “<script type="math/tex">w_a\text{ is to }w_{a^*}\text{ as }w_b\text{ is to }w_{b^*}</script>” implies that <script type="math/tex">\{w_b, w_{a^*}\!\}</script> paraphrases <script type="math/tex">\{w_a, w_{b^*}\!\}</script>.
We show that these conditions are in fact equivalent by reinterpreting paraphrases as <em>word transformations</em>.</p>

<h3 id="word-transformation">Word Transformation</h3>

<p>From <a href="#paraphrases">above</a>, the paraphrase of a word set <script type="math/tex">\mathcal{W}</script> by a word <script type="math/tex">w_*</script> can be thought of as drawing a semantic equivalence between <script type="math/tex">\mathcal{W}</script> and <script type="math/tex">w_*</script>. Alternatively, we can choose a particular word $w$ in $\mathcal{W}$ and view the paraphrase as indicating words (i.e. all others in <script type="math/tex">\mathcal{W}</script>, denoted <script type="math/tex">\mathcal{W}^+</script>) that, when added to <script type="math/tex">w</script>, make it “more like” – or <em>transform</em> it to – <script type="math/tex">w_*</script>. For example, the paraphrase of <script type="math/tex">\{man, royal\}</script>  by <script type="math/tex">king</script> can be interpreted as a <em>word transformation</em> from <script type="math/tex">man</script> to <script type="math/tex">king</script> by adding <script type="math/tex">royal</script>. In effect, the added words <em>narrow the context</em>. More precisely, they alter the distribution of context words found around <script type="math/tex">w</script> to more closely align with that of <script type="math/tex">w_*</script>. Denoting a paraphrase by <script type="math/tex">\approx_p</script>, we can represent this as:</p>

<p style="text-align: center"><img src="/assets/word_trans_1.png" alt="word transformation" width="300px" /></p>

<p>in which the paraphrase can be seen as the “glue” in a relationship between, or rather, <em>from</em> <script type="math/tex">w</script> <em>to</em> <script type="math/tex">w_*</script>.
Thus, if <script type="math/tex">w\!\in\!\mathcal{W}</script> and <script type="math/tex">\mathcal{W}^+ \!=\! \mathcal{W}\!\setminus\!\!\{w\}</script>, to say “<script type="math/tex">w_*</script> paraphrases <script type="math/tex">\mathcal{W}</script>” is equivalent to saying “there exists a word transformation from <script type="math/tex">w</script> to <script type="math/tex">w_*</script> by adding <script type="math/tex">\mathcal{W}^+</script>”.
To be clear, nothing changes other than perspective.</p>

<p>Extending the concept to paraphrases between word sets <script type="math/tex">\mathcal{W}</script>, <script type="math/tex">\mathcal{W}_*\!</script>, we can choose any <script type="math/tex">w \!\in\! \mathcal{W}</script>, <script type="math/tex">w_* \!\in\! \mathcal{W}_*</script> and view the paraphrase as defining a relationship between <script type="math/tex">w</script> and <script type="math/tex">w_*</script> in which <script type="math/tex">\mathcal{W}^+ \!=\! \mathcal{W}\!\setminus\!\!\{w\}</script> is added to <script type="math/tex">w</script> and <script type="math/tex">\mathcal{W}^- \!=\! \mathcal{W_*}\!\!\setminus\!\!\{w_*\}</script> to <script type="math/tex">w_*</script>:</p>

<p style="text-align: center"><img src="/assets/word_trans_2.png" alt="word transformation" width="300px" /></p>

<p>This is not a word transformation as above, since it lacks the same notion of direction <em>from</em> <script type="math/tex">w</script> <em>to</em> <script type="math/tex">w_*</script>. To remedy this, rather than considering the words in <script type="math/tex">\mathcal{W}^-</script> as being added to <script type="math/tex">w_*</script>, we consider them <em>subtracted</em> from <script type="math/tex">w</script> (hence the naming convention):</p>

<p style="text-align: center"><img src="/assets/word_trans_3.png" alt="word transformation" width="300px" /></p>

<p>Where added words narrow context, subtracted words can be thought of as <em>broadening</em> the context.</p>

<blockquote>
  <p>We say there exists a <strong><em>word transformation</em></strong> from word <script type="math/tex">w</script> to word <script type="math/tex">w_*</script>, with <strong><em>transformation parameters</em></strong> <script type="math/tex">\mathcal{W}^+\!\!</script>, <script type="math/tex">\mathcal{W}^-\!\subseteq\mathcal{E}\</script>    <em>iff</em>   <script type="math/tex">\ \{w\}\!\cup\!\mathcal{W}^+ \approx_\text{P} \{w_*\}\!\cup\!\mathcal{W}^-</script>.</p>
</blockquote>

<h4 id="intuition">Intuition</h4>

<p>The intuition behind word transformations mirrors simple algebra, e.g. 8 is made <em>equivalent</em> to 5 by adding 3 to the right, or subtracting 3 from the left. Analogously, with paraphrasing as a measure of <em>equivalence</em>, we can identify words (<script type="math/tex">\mathcal{W}^+, \mathcal{W}^-</script>) that when added to/subtracted from <script type="math/tex">w</script>, make it equivalent to <script type="math/tex">w_*</script>.
In doing so, just as 3 describes the difference between 8 and 5 in the numeric example, we find words that <em>describe the difference</em> between <script type="math/tex">w</script> and <script type="math/tex">w_*</script>, or rather, how “<script type="math/tex">w</script> is to <script type="math/tex">w_*</script>”.</p>

<p>With our initial paraphrases, words could only be <em>added</em> to <script type="math/tex">w</script> to describe its semantic difference to <script type="math/tex">w_*</script>, limiting the tools available to discrete words in <script type="math/tex">\mathcal{E}</script>. Now, <em>differences between other words</em> can also be used offering a far richer toolkit, e.g. the difference between <script type="math/tex">man</script> and <script type="math/tex">king</script> can crudely be explained by, say, <script type="math/tex">royal</script> or $crown$, but can more closely be described by the difference between $woman$ and $queen$.</p>

<h3 id="interpreting-analogies">Interpreting Analogies</h3>

<p>We can now mathematically interpret the language of an analogy:</p>

<blockquote>
  <p>We say <em><script type="math/tex">\ `\!`w_a</script> is to <script type="math/tex">w_{a^*}</script> as <script type="math/tex">w_b</script> is to <script type="math/tex">w_{b^*}\!</script>”</em> <em>iff</em> there exist <script type="math/tex">\mathcal{W}^+\!, \mathcal{W}^-\!\subseteq\!\mathcal{E}</script> that serve as transformation parameters to transform both <script type="math/tex">w_a</script> to <script type="math/tex">w_{a^*}</script> and <script type="math/tex">w_b</script> to <script type="math/tex">w_{b^*}</script>.</p>
</blockquote>

<p>That is, within the analogy wording, each instance of “is to” refers to the parameters of a word transformation and “as” implies their equality. Thus the semantic differences  within each word pair, as captured by the transformation parameters, are the same – fitting intuition and now defined explicitly.</p>

<p>So, an analogy is a pair of word transformations with common parameters <script type="math/tex">\mathcal{W}^+, \mathcal{W}^-</script>, but what are those parameters? Fortunately, we need not search or guess. We show in the <a href="https://arxiv.org/abs/1901.09813">paper</a> (Sec 6.4) that if an analogy holds, then <em>any</em> parameters that transform one word pair, e.g. <script type="math/tex">w_a</script> to <script type="math/tex">w_{a^*}</script>, <em>must also transform the other pair</em>.
As such, we can chose <script type="math/tex">\mathcal{W}^+\!=\!\{w_{a^*}\!\}</script>, <script type="math/tex">\mathcal{W}^-\!=\!\{w_{a}\}</script>, which perfectly transform <script type="math/tex">w_a</script> to <script type="math/tex">w_{a^*}</script> since <script type="math/tex">\{w_a, w_{a^*}\!\}</script> paraphrases <script type="math/tex">\{w_{a^*\!}, w_a\}</script> exactly (note that ordering is irrelevant in paraphrases, e.g. <script type="math/tex">\{man</script>, <script type="math/tex">royal\}</script> paraphrases  <script type="math/tex">\{royal</script>, <script type="math/tex">man\}</script>). But, if the analogy holds, those same parameters must also transform <script type="math/tex">w_b</script> to <script type="math/tex">w_{b^*}</script>, meaning that <script type="math/tex">\{w_b, w_{a^*}\}</script> paraphrases <script type="math/tex">\{w_{b^*}, w_a\}</script>.</p>

<table class="mbtablestyle">
  <tbody>
    <tr>
      <td>Thus, <em><script type="math/tex">`\!`w_a</script> is to <script type="math/tex">w_{a^*}</script> as <script type="math/tex">w_b</script> is to <script type="math/tex">w_{b^*}\!\!"</script></em> <script type="math/tex">\</script> if and only if <script type="math/tex">\</script> <script type="math/tex">\{w_b, w_{a^*}\}</script> paraphrases <script type="math/tex">\{w_{b^*}, w_a\}</script>.</td>
    </tr>
  </tbody>
</table>

<p>This completes the chain:</p>
<ul>
  <li>analogies are equivalent to word transformations with common transformation parameters that describe the common semantic difference;</li>
  <li>those word transformations are equivalent to paraphrases, the first of which is rendered trivial under a particular choice of transformation parameters;</li>
  <li>the second paraphrase leads to a geometric relationship between PMI vectors \eqref{eq:two}, subject to the accuracy of the paraphrase (<script type="math/tex">\rho</script>) and dependence error terms (<script type="math/tex">\sigma, \tau</script>); and</li>
  <li>under low-dimensional projection (induced by the loss function), the same geometric relationship manifests in word embeddings of analogies \eqref{eq:one}, as seen in word embeddings of W2V and Glove.</li>
</ul>

<p>Returning to an earlier plot, we can now explain the “gap” in terms of paraphrase (<script type="math/tex">\rho</script>) and dependence <script type="math/tex">(\sigma, \tau</script>) error terms, and understand why it is small, often smallest, for the word completing the analogy.</p>

<p style="text-align: center"><img src="/assets/solution.png" alt="embedding gap explained" height="250px" /></p>

<hr />
<hr />
<p><br /></p>

<h2 id="related-work">Related Work</h2>
<p>Several other works aim to explain the analogy phenomenon:</p>
<ul>
  <li><a href="https://aclweb.org/anthology/Q16-1028">Arora et al. (2016)</a> propose a latent variable model for text generation that is claimed <em>inter alia</em> to explain analogies, however strong <em>a priori</em> assumptions are made about the arrangement of word vectors that we do not require. More recently, <a href="https://arxiv.org/abs/1805.12164">we have shown</a> that certain results of this work contradict the relationship between W2V embeddings and PMI (<a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Levy &amp; Goldberg</a>).</li>
  <li><a href="https://www.aclweb.org/anthology/P17-1007">Gittens et al. (2017)</a> introduce the idea of paraphrasing to explain analogies, from which we draw inspiration, but they include several assumptions that fail in practice, in particular that word frequencies follow a uniform distribution rather than their actual, highly non-uniform Zipf distribution.</li>
  <li><a href="https://arxiv.org/pdf/1810.04882v6.pdf">Ethayarajh et al. (2019)</a> look to show that word embeddings of analogies form parallelograms by considering the latter’s geometric properties. However:
 (i) that all points must be co-planar is assumed without explanation;
 (ii) that opposite sides must have similar direction is omitted (as such, a “bow-tie” shape satisfies their Lemma 1); and
 (iii) that opposite sides must have similar Euclidean distance is translated to a statistic “csPMI” erroneously (since they rely on the strong relationship between embedding matrices <script type="math/tex">\mathbf{W} \!=\! \lambda\mathbf{C}</script> for some <script type="math/tex">\lambda \!\in\! \mathbb{R}</script>, which is false) and without connection to analogies or semantics.</li>
</ul>

<p>As such, we provide the first end-to-end explanation for the geometric relationship between word embeddings observed for analogies.</p>

<hr />
<hr />
<p><br /></p>

<h2 id="further-work">Further Work</h2>

<p>Two recent works build on this paper:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1805.12164">“What the Vec? Towards Probabilistically Grounded Embeddings”</a> extends the principles of this work to show how W2V and Glove (approximately) capture other relationships such as <em>relatedness</em> and <em>similarity</em>, what certain embedding interactions correspond to and, in doing so, how the semantic relationships of <em>relatedness</em>, <em>similarity</em>, <em>paraphrasing</em> and <em>analogies</em> mathematically inter-relate.</li>
  <li><a href="https://arxiv.org/abs/1905.09791">“Multi-relational Poincaré Graph Embeddings”</a> <a href="https://github.com/ibalazevic/multirelational-poincare">[code]</a> draws a comparison between analogies and <em>relations</em> in Knowledge Graphs to develop state-of-the-art representation models in both Euclidean and Hyperbolic space.</li>
</ul>


  </div><a class="u-url" href="/nlp/2019/07/01/explaining-analogies-explained.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

<!--
    <h2 class="footer-heading">Carl Allen: Machine Learning Blog</h2>
-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Carl Allen: Machine Learning Blog</li><li><a class="u-email" href="mailto:carl.allen@ed.ac.uk">carl.allen@ed.ac.uk</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/carl-allen"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">carl-allen</span></a></li><li><a href="https://www.twitter.com/carl_s_allen"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">carl_s_allen</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>PhD student, University of Edinburgh
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
