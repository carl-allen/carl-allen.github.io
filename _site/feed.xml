<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Carl Allen: Machine Learning Blog</title>
    <description>PhD student, University of Edinburgh
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 06 Jul 2019 19:28:50 +0100</pubDate>
    <lastBuildDate>Sat, 06 Jul 2019 19:28:50 +0100</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>''Analogies Explained'' ... Explained</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;This post provides a `less maths, more intuition’ overview of &lt;a href=&quot;https://arxiv.org/abs/1901.09813&quot;&gt;Analogies Explained: Towards Understanding Word Embeddings&lt;/a&gt; (ICML, 2019, Best Paper Honourable Mention). The outline follows that of the &lt;a href=&quot;https://icml.cc/media/Slides/icml/2019/104(13-11-00)-13-11-00-4883-analogies_expla.pdf&quot;&gt;conference presentation&lt;/a&gt;. Target audience: general machine learning, NLP, computational linguistics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;word-embeddings&quot;&gt;Word Embeddings&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Word embeddings&lt;/em&gt; are numerical vector representations of words. Each entry, or dimension, of a word embedding can be thought of as capturing some semantic or syntactic feature of the word, and the full vector can be considered as co-ordinates of the word in a high-dimensional space. Word embeddings can be generated explicitly, e.g. from rows of word co-occurrence statistics (or low-rank approximations of such statistics); or by &lt;em&gt;neural network&lt;/em&gt; methods such as &lt;em&gt;&lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;Word2Vec&lt;/a&gt;&lt;/em&gt; (W2V) or &lt;em&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot;&gt;Glove&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The latter, &lt;em&gt;neural word embeddings&lt;/em&gt;, are found to be highly useful in natural language processing (NLP) tasks, such as evaluating word similarity, identifying named entities and assessing positive or negative sentiment in a passage of text (e.g. a customer review).&lt;/p&gt;

&lt;h3 id=&quot;analogies&quot;&gt;Analogies&lt;/h3&gt;

&lt;p&gt;An intriguing property of neural word embeddings is that &lt;em&gt;analogies&lt;/em&gt; can often be solved simply by adding/subtracting word embeddings. For example, the classic analogy:&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;``man&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;king&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;woman&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;queen\!&quot;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;can be &lt;em&gt;solved&lt;/em&gt; using word embeddings by finding that closest to &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman}&lt;/script&gt;, which turns out to be &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{queen}&lt;/script&gt;
($\mathbf{w}_{x}$ denotes the embedding of word $x$). Note that words in the question (e.g. &lt;script type=&quot;math/tex&quot;&gt;man&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;woman&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;king&lt;/script&gt;) are typically omitted from the search.
This suggests the relationship:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \mathbf{w}_{king} - \mathbf{w}_{man} + \mathbf{w}_{woman} \approx \mathbf{w}_{queen}\,
  \tag{1}\label{eq:one}
\end{equation}&lt;/script&gt;

&lt;p&gt;or, in geometric terms, that &lt;strong&gt;&lt;em&gt;word embeddings of analogies approximately form parallelograms:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/parallelogram2.svg&quot; alt=&quot;word embeddings of analogies approximately form a paralellogram&quot; height=&quot;170px&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;Whilst this fits our intuition, the phenomenon is intriguing since word embeddings are not trained to achieve it!
In practice, word embeddings of analogies do not &lt;em&gt;perfectly&lt;/em&gt; form parallelograms:&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/analogy_embeddings.png&quot; alt=&quot;word embeddings of analogies not quite a paralellogram&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This shows the (exact) parallelogram formed by &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{king}, \mathbf{w}_{man}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{woman}&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{king} {\small-} \mathbf{w}_{man} {\small+} \mathbf{w}_{woman}&lt;/script&gt; fixed in the &lt;script type=&quot;math/tex&quot;&gt;xy&lt;/script&gt;-plane and a selection of word embeddings shown relative to it. We see that the embedding of &lt;script type=&quot;math/tex&quot;&gt;queen&lt;/script&gt; does not sit at its corner, but is the closest to it. Word embeddings of related words, e.g. &lt;script type=&quot;math/tex&quot;&gt;prince&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;lord&lt;/script&gt;, lie relatively close by and random unrelated words are further away.&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;We explain the relationship between word embeddings of analogies \eqref{eq:one}, by explaining the gap (indicated) between &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{queen}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_{king} {\small-} \mathbf{w}_{man} {\small+} \mathbf{w}_{woman}&lt;/script&gt;; and why it is small, often smallest, for the word that completes the analogy.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To understand why &lt;em&gt;semantic&lt;/em&gt; relationships between words give rise to &lt;em&gt;geometric&lt;/em&gt; relationships between word embeddings, we first consider what W2V embeddings learn.&lt;/p&gt;

&lt;h3 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/neural_network.png&quot; alt=&quot;W2V architecture&quot; style=&quot;float: right; margin-right: 0px; margin-top: -10px;&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;W2V (SkipGram with negative sampling) is an algorithm that generates word embeddings by training the weights of a 2-layer “neural network”
to predict &lt;em&gt;context words&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;c_j&lt;/script&gt; (i.e. words that fall within a context window of fixed size &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;) around each word  &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; (referred to as a &lt;em&gt;target word&lt;/em&gt;) across a text corpus.&lt;/p&gt;

&lt;p&gt;Predicting &lt;script type=&quot;math/tex&quot;&gt;p(c_j\!\mid\! w_i)&lt;/script&gt;, for all &lt;script type=&quot;math/tex&quot;&gt;c_j&lt;/script&gt; in a dictionary of all unique words &lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}&lt;/script&gt;, was initially considered with a softmax function, but instead a sigmoid function and negative sampling were used to reduce computational cost.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf&quot;&gt;Levy &amp;amp; Goldberg (2014)&lt;/a&gt; showed that, as a result, the weight matrices &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}, \mathbf{C}&lt;/script&gt; (columns of which are the word embeddings &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_i, \mathbf{c}_j&lt;/script&gt;) approximately factorise a matrix of &lt;em&gt;shifted&lt;/em&gt; Pointwise Mutual Information (PMI):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathbf{W}^\top\mathbf{C} \approx \textbf{PMI} - \log k\ ,
\end{equation}&lt;/script&gt;

&lt;p&gt;where $k$ is the chosen number of negative samples and
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
\textbf{PMI}_{i,j} = \text{PMI}(w_i, c_j) = \log\tfrac{p(w_i,\, c_j)}{p(w_i)p(c_j)}
\end{equation}\,.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Dropping the &lt;em&gt;shift&lt;/em&gt; term “&lt;script type=&quot;math/tex&quot;&gt;\log k&lt;/script&gt;”, an artefact of the W2V algorithm (that we reconsider in the &lt;a href=&quot;https://arxiv.org/abs/1901.09813&quot;&gt;paper&lt;/a&gt;, Sec 5.5, 6.8), the relationship
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation}
\mathbf{W}^\top\!\mathbf{C} \!\approx\! \textbf{PMI}
\end{equation}&lt;/script&gt;
shows that an embedding &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}_i&lt;/script&gt; can be considered a &lt;em&gt;low-dimensional projection&lt;/em&gt; of a row of the PMI matrix, &lt;script type=&quot;math/tex&quot;&gt;\text{PMI}_i&lt;/script&gt; (a &lt;em&gt;PMI vector&lt;/em&gt;).&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p id=&quot;proof&quot;&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;proving-the-embedding-relationship-of-analogies&quot;&gt;Proving the embedding relationship of analogies&lt;/h2&gt;

&lt;p&gt;From above, it can be seen that the additive relationship between word embeddings of analogies \eqref{eq:one} follows if
(a) an equivalent relationship exists between PMI vectors, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{PMI}_{king} - \text{PMI}_{man} + \text{PMI}_{woman} \approx \text{PMI}_{queen}\ ;
  \tag{2}\label{eq:two}
\end{equation}&lt;/script&gt;

&lt;p&gt;and (b) vector addition is sufficiently preserved under the low-rank projection induced by the loss function, as readily achieved by a least squares loss function and as approximated by W2V and Glove.&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;To prove that relationship \eqref{eq:two} arises between PMI vectors of an analogy, we show that \eqref{eq:two} follows from a particular &lt;strong&gt;paraphrase&lt;/strong&gt; relationship, which is, in turn, shown to be equivalent to an analogy.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;paraphrases&quot;&gt;Paraphrases&lt;/h3&gt;

&lt;p&gt;When we say a word &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; &lt;em&gt;paraphrases&lt;/em&gt; a set of words &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt;, we mean, intuitively, that they are &lt;em&gt;semantically interchangeable&lt;/em&gt; in the text. For example, where &lt;script type=&quot;math/tex&quot;&gt;king&lt;/script&gt; appears, we might instead see  &lt;script type=&quot;math/tex&quot;&gt;man&lt;/script&gt; &lt;u&gt;and&lt;/u&gt;  &lt;script type=&quot;math/tex&quot;&gt;royal&lt;/script&gt; close together.
Mathematically, a best choice paraphrase word &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; can be defined as that which maximises the likelihood of the context words observed around &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt;.
In other words, the distribution of words observed around $w_*$ defined over the dictionary &lt;script type=&quot;math/tex&quot;&gt;p(\mathcal{E}\!\mid\!w_*)&lt;/script&gt;, should be similar to that around &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;p(\mathcal{E}\!\mid\!\mathcal{W})&lt;/script&gt;, as measured by Kullback-Leibler (KL) divergence. Whilst these distributions are discrete and unordered, we can picture this as:&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/distributions.png&quot; alt=&quot;Paraphrase distributions&quot; height=&quot;120px&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Formally, we say &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; &lt;strong&gt;paraphrases&lt;/strong&gt; $\mathcal{W}$ if the &lt;strong&gt;paraphrase error&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;{\rho}^{\mathcal{W}, w_*}\!\in\! \mathbb{R}^{n}&lt;/script&gt;
 is (element-wise) small, where:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
    {\rho}^{\mathcal{W}, w_*}_j = \log \tfrac{p(c_j|w_*)}{p(c_j|\mathcal{W})}\ ,   \quad c_j\!\in\!\mathcal{E}.
\end{equation}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see the relevance of paraphrases, we compare the sum of PMI vectors of two words &lt;script type=&quot;math/tex&quot;&gt;w_1, w_2&lt;/script&gt; to that of &lt;strong&gt;&lt;u&gt;any word&lt;/u&gt;&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; by considering each (&lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt;) component of the difference vector &lt;script type=&quot;math/tex&quot;&gt;\text{PMI}_* - (\text{PMI}_1 + \text{PMI}_2)&lt;/script&gt;:&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/equation_PMI.png&quot; alt=&quot;PMI relationship&quot; height=&quot;230px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that the difference can be written as a &lt;em&gt;paraphrase error&lt;/em&gt;, small only if &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\{w_1, w_2\}&lt;/script&gt;, and &lt;em&gt;dependence error&lt;/em&gt; terms inherent to &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt; that &lt;strong&gt;&lt;u&gt;do not depend on $w_*$&lt;/u&gt;&lt;/strong&gt;.
Formally, we have:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lemma 1:
For any word &lt;script type=&quot;math/tex&quot;&gt;w_*\!\in\!\mathcal{E}&lt;/script&gt; and word set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}\!\subseteq\!\mathcal{E}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
|\mathcal{W}|\!&lt;\!l %]]&gt;&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; is the context window size:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
    \text{PMI}_*
    \ =\
    \sum_{w_i\in\mathcal{W}} \text{PMI}_{i}
     \,+\, \rho^{\mathcal{W}, w_*}
     \,+\, \sigma^{\mathcal{W}}
     \,-\, \tau^\mathcal{W}\mathbf{1}\ .
\end{equation}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;This connects paraphrasing to PMI vector &lt;em&gt;addition&lt;/em&gt;, as appears in \eqref{eq:two}. To develop this further, paraphrasing is generalised, replacing &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}_*&lt;/script&gt;, to a relationship between any &lt;em&gt;two word sets&lt;/em&gt;.
The underlying principle remains the same: word sets paraphrase one another if the distributions of context words around them are similar (indeed, the original paraphrase definition is recovered if &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}_*&lt;/script&gt; contains a single word). Analogously to above, we find:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lemma 2:
For any word sets &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}, \mathcal{W}_*\!\subseteq\!\mathcal{E}&lt;/script&gt;; &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
|\mathcal{W}|, |\mathcal{W}_*|\!&lt;\!l %]]&gt;&lt;/script&gt;:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
    \sum_{w_i\in\mathcal{W}_*} \text{PMI}_{i}
    \ =\
    \sum_{w_i\in\mathcal{W}} \text{PMI}_{i}
     \,+\, \rho^{\mathcal{W}, \mathcal{W}_*}
     \,+\, (\sigma^{\mathcal{W}} - \sigma^{\mathcal{W}_*})
     \,-\, (\tau^\mathcal{W} - \tau^{\mathcal{W}_*})\mathbf{1}
\end{equation}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can apply this to our example by setting &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W} \!=\! \{woman, king\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}_* \!=\!  \{man, queen\}&lt;/script&gt;, whereby&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
  \text{PMI}_{king} - \text{PMI}_{man} + \text{PMI}_{woman} \ \approx\ \text{PMI}_{queen}\ ,
\end{equation}&lt;/script&gt;

&lt;p&gt;if &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}_*&lt;/script&gt; (meaning &lt;script type=&quot;math/tex&quot;&gt;\rho^{\mathcal{W}, \mathcal{W}_*}&lt;/script&gt; is small), subject to &lt;em&gt;net&lt;/em&gt; statistical dependencies of words within &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}_*&lt;/script&gt;, i.e. &lt;script type=&quot;math/tex&quot;&gt;\sigma^{\mathcal{W}} - \sigma^{\mathcal{W}_*}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\tau^\mathcal{W} - \tau^{\mathcal{W}_*}&lt;/script&gt;.&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Thus, \eqref{eq:two} holds, subject to dependence error, if &lt;script type=&quot;math/tex&quot;&gt;\{woman, king\}&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\{man, queen\}&lt;/script&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This establishes a semantic relationship as a sufficient condition for the geometric relationship we aim to explain – but that semantic relationship is not an analogy.
What remains then, is to explain why a general analogy “&lt;script type=&quot;math/tex&quot;&gt;w_a\text{ is to }w_{a^*}\text{ as }w_b\text{ is to }w_{b^*}&lt;/script&gt;” implies that &lt;script type=&quot;math/tex&quot;&gt;\{w_b, w_{a^*}\!\}&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\{w_a, w_{b^*}\!\}&lt;/script&gt;.
We show that these conditions are in fact equivalent by reinterpreting paraphrases as &lt;em&gt;word transformations&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;word-transformation&quot;&gt;Word Transformation&lt;/h3&gt;

&lt;p&gt;From &lt;a href=&quot;#paraphrases&quot;&gt;above&lt;/a&gt;, the paraphrase of a word set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt; by a word &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; can be thought of as drawing a semantic equivalence between &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;. Alternatively, we can choose a particular word $w$ in $\mathcal{W}$ and view the paraphrase as indicating words (i.e. all others in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt;, denoted &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+&lt;/script&gt;) that, when added to &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;, make it “more like” – or &lt;em&gt;transform&lt;/em&gt; it to – &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;. For example, the paraphrase of &lt;script type=&quot;math/tex&quot;&gt;\{man, royal\}&lt;/script&gt;  by &lt;script type=&quot;math/tex&quot;&gt;king&lt;/script&gt; can be interpreted as a &lt;em&gt;word transformation&lt;/em&gt; from &lt;script type=&quot;math/tex&quot;&gt;man&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;king&lt;/script&gt; by adding &lt;script type=&quot;math/tex&quot;&gt;royal&lt;/script&gt;. In effect, the added words &lt;em&gt;narrow the context&lt;/em&gt;. More precisely, they alter the distribution of context words found around &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; to more closely align with that of &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;. Denoting a paraphrase by &lt;script type=&quot;math/tex&quot;&gt;\approx_p&lt;/script&gt;, we can represent this as:&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/word_trans_1.png&quot; alt=&quot;word transformation&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;in which the paraphrase can be seen as the “glue” in a relationship between, or rather, &lt;em&gt;from&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; &lt;em&gt;to&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;.
Thus, if &lt;script type=&quot;math/tex&quot;&gt;w\!\in\!\mathcal{W}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+ \!=\! \mathcal{W}\!\setminus\!\!\{w\}&lt;/script&gt;, to say “&lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt;” is equivalent to saying “there exists a word transformation from &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; by adding &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+&lt;/script&gt;”.
To be clear, nothing changes other than perspective.&lt;/p&gt;

&lt;p&gt;Extending the concept to paraphrases between word sets &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}_*\!&lt;/script&gt;, we can choose any &lt;script type=&quot;math/tex&quot;&gt;w \!\in\! \mathcal{W}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;w_* \!\in\! \mathcal{W}_*&lt;/script&gt; and view the paraphrase as defining a relationship between &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt; in which &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+ \!=\! \mathcal{W}\!\setminus\!\!\{w\}&lt;/script&gt; is added to &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^- \!=\! \mathcal{W_*}\!\!\setminus\!\!\{w_*\}&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;:&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/word_trans_2.png&quot; alt=&quot;word transformation&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is not a word transformation as above, since it lacks the same notion of direction &lt;em&gt;from&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; &lt;em&gt;to&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;. To remedy this, rather than considering the words in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^-&lt;/script&gt; as being added to &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;, we consider them &lt;em&gt;subtracted&lt;/em&gt; from &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; (hence the naming convention):&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/word_trans_3.png&quot; alt=&quot;word transformation&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where added words narrow context, subtracted words can be thought of as &lt;em&gt;broadening&lt;/em&gt; the context.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We say there exists a &lt;strong&gt;&lt;em&gt;word transformation&lt;/em&gt;&lt;/strong&gt; from word &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; to word &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;, with &lt;strong&gt;&lt;em&gt;transformation parameters&lt;/em&gt;&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+\!\!&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^-\!\subseteq\mathcal{E}\&lt;/script&gt;    &lt;em&gt;iff&lt;/em&gt;   &lt;script type=&quot;math/tex&quot;&gt;\ \{w\}\!\cup\!\mathcal{W}^+ \approx_\text{P} \{w_*\}\!\cup\!\mathcal{W}^-&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;intuition&quot;&gt;Intuition&lt;/h4&gt;

&lt;p&gt;The intuition behind word transformations mirrors simple algebra, e.g. 8 is made &lt;em&gt;equivalent&lt;/em&gt; to 5 by adding 3 to the right, or subtracting 3 from the left. Analogously, with paraphrasing as a measure of &lt;em&gt;equivalence&lt;/em&gt;, we can identify words (&lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+, \mathcal{W}^-&lt;/script&gt;) that when added to/subtracted from &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;, make it equivalent to &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;.
In doing so, just as 3 describes the difference between 8 and 5 in the numeric example, we find words that &lt;em&gt;describe the difference&lt;/em&gt; between &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;, or rather, how “&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;”.&lt;/p&gt;

&lt;p&gt;With our initial paraphrases, words could only be &lt;em&gt;added&lt;/em&gt; to &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; to describe its semantic difference to &lt;script type=&quot;math/tex&quot;&gt;w_*&lt;/script&gt;, limiting the tools available to discrete words in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}&lt;/script&gt;. Now, &lt;em&gt;differences between other words&lt;/em&gt; can also be used offering a far richer toolkit, e.g. the difference between &lt;script type=&quot;math/tex&quot;&gt;man&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;king&lt;/script&gt; can crudely be explained by, say, &lt;script type=&quot;math/tex&quot;&gt;royal&lt;/script&gt; or $crown$, but can more closely be described by the difference between $woman$ and $queen$.&lt;/p&gt;

&lt;h3 id=&quot;interpreting-analogies&quot;&gt;Interpreting Analogies&lt;/h3&gt;

&lt;p&gt;We can now mathematically interpret the language of an analogy:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We say &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;\ `\!`w_a&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;w_{a^*}&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;w_b&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;w_{b^*}\!&lt;/script&gt;”&lt;/em&gt; &lt;em&gt;iff&lt;/em&gt; there exist &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+\!, \mathcal{W}^-\!\subseteq\!\mathcal{E}&lt;/script&gt; that serve as transformation parameters to transform both &lt;script type=&quot;math/tex&quot;&gt;w_a&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_{a^*}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_b&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_{b^*}&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That is, within the analogy wording, each instance of “is to” refers to the parameters of a word transformation and “as” implies their equality. Thus the semantic differences  within each word pair, as captured by the transformation parameters, are the same – fitting intuition and now defined explicitly.&lt;/p&gt;

&lt;p&gt;So, an analogy is a pair of word transformations with common parameters &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+, \mathcal{W}^-&lt;/script&gt;, but what are those parameters? Fortunately, we need not search or guess. We show in the &lt;a href=&quot;https://arxiv.org/abs/1901.09813&quot;&gt;paper&lt;/a&gt; (Sec 6.4) that if an analogy holds, then &lt;em&gt;any&lt;/em&gt; parameters that transform one word pair, e.g. &lt;script type=&quot;math/tex&quot;&gt;w_a&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_{a^*}&lt;/script&gt;, &lt;em&gt;must also transform the other pair&lt;/em&gt;.
As such, we can chose &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^+\!=\!\{w_{a^*}\!\}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{W}^-\!=\!\{w_{a}\}&lt;/script&gt;, which perfectly transform &lt;script type=&quot;math/tex&quot;&gt;w_a&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_{a^*}&lt;/script&gt; since &lt;script type=&quot;math/tex&quot;&gt;\{w_a, w_{a^*}\!\}&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\{w_{a^*\!}, w_a\}&lt;/script&gt; exactly (note that ordering is irrelevant in paraphrases, e.g. &lt;script type=&quot;math/tex&quot;&gt;\{man&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;royal\}&lt;/script&gt; paraphrases  &lt;script type=&quot;math/tex&quot;&gt;\{royal&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;man\}&lt;/script&gt;). But, if the analogy holds, those same parameters must also transform &lt;script type=&quot;math/tex&quot;&gt;w_b&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;w_{b^*}&lt;/script&gt;, meaning that &lt;script type=&quot;math/tex&quot;&gt;\{w_b, w_{a^*}\}&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\{w_{b^*}, w_a\}&lt;/script&gt;.&lt;/p&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Thus, &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;`\!`w_a&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;w_{a^*}&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;w_b&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;w_{b^*}\!\!&quot;&lt;/script&gt;&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\&lt;/script&gt; if and only if &lt;script type=&quot;math/tex&quot;&gt;\&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\{w_b, w_{a^*}\}&lt;/script&gt; paraphrases &lt;script type=&quot;math/tex&quot;&gt;\{w_{b^*}, w_a\}&lt;/script&gt;.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This completes the chain:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;analogies are equivalent to word transformations with common transformation parameters that describe the common semantic difference;&lt;/li&gt;
  &lt;li&gt;those word transformations are equivalent to paraphrases, the first of which is rendered trivial under a particular choice of transformation parameters;&lt;/li&gt;
  &lt;li&gt;the second paraphrase leads to a geometric relationship between PMI vectors \eqref{eq:two}, subject to the accuracy of the paraphrase (&lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;) and dependence error terms (&lt;script type=&quot;math/tex&quot;&gt;\sigma, \tau&lt;/script&gt;); and&lt;/li&gt;
  &lt;li&gt;under low-dimensional projection (induced by the loss function), the same geometric relationship manifests in word embeddings of analogies \eqref{eq:one}, as seen in word embeddings of W2V and Glove.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Returning to an earlier plot, we can now explain the “gap” in terms of paraphrase (&lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;) and dependence &lt;script type=&quot;math/tex&quot;&gt;(\sigma, \tau&lt;/script&gt;) error terms, and understand why it is small, often smallest, for the word completing the analogy.&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/solution.png&quot; alt=&quot;embedding gap explained&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;Several other works aim to explain the analogy phenomenon:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclweb.org/anthology/Q16-1028&quot;&gt;Arora et al. (2016)&lt;/a&gt; propose a latent variable model for text generation that is claimed &lt;em&gt;inter alia&lt;/em&gt; to explain analogies, however strong &lt;em&gt;a priori&lt;/em&gt; assumptions are made about the arrangement of word vectors that we do not require. More recently, &lt;a href=&quot;https://arxiv.org/abs/1805.12164&quot;&gt;we have shown&lt;/a&gt; that certain results of this work contradict the relationship between W2V embeddings and PMI (&lt;a href=&quot;https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf&quot;&gt;Levy &amp;amp; Goldberg&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P17-1007&quot;&gt;Gittens et al. (2017)&lt;/a&gt; introduce the idea of paraphrasing to explain analogies, from which we draw inspiration, but they include several assumptions that fail in practice, in particular that word frequencies follow a uniform distribution rather than their actual, highly non-uniform Zipf distribution.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.04882v6.pdf&quot;&gt;Ethayarajh et al. (2019)&lt;/a&gt; look to show that word embeddings of analogies form parallelograms by considering the latter’s geometric properties. However:
 (i) that all points must be co-planar is assumed without explanation;
 (ii) that opposite sides must have similar direction is omitted (as such, a “bow-tie” shape satisfies their Lemma 1); and
 (iii) that opposite sides must have similar Euclidean distance is translated to a statistic “csPMI” erroneously (since they rely on the strong relationship between embedding matrices &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W} \!=\! \lambda\mathbf{C}&lt;/script&gt; for some &lt;script type=&quot;math/tex&quot;&gt;\lambda \!\in\! \mathbb{R}&lt;/script&gt;, which is false) and without connection to analogies or semantics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As such, we provide the first end-to-end explanation for the geometric relationship between word embeddings observed for analogies.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;further-work&quot;&gt;Further Work&lt;/h2&gt;

&lt;p&gt;Two recent works build on this paper:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.12164&quot;&gt;“What the Vec? Towards Probabilistically Grounded Embeddings”&lt;/a&gt; extends the principles of this work to show how W2V and Glove (approximately) capture other relationships such as &lt;em&gt;relatedness&lt;/em&gt; and &lt;em&gt;similarity&lt;/em&gt;, what certain embedding interactions correspond to and, in doing so, how the semantic relationships of &lt;em&gt;relatedness&lt;/em&gt;, &lt;em&gt;similarity&lt;/em&gt;, &lt;em&gt;paraphrasing&lt;/em&gt; and &lt;em&gt;analogies&lt;/em&gt; mathematically inter-relate.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.09791&quot;&gt;“Multi-relational Poincaré Graph Embeddings”&lt;/a&gt; &lt;a href=&quot;https://github.com/ibalazevic/multirelational-poincare&quot;&gt;[code]&lt;/a&gt; draws a comparison between analogies and &lt;em&gt;relations&lt;/em&gt; in Knowledge Graphs to develop state-of-the-art representation models in both Euclidean and Hyperbolic space.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 01 Jul 2019 15:44:00 +0100</pubDate>
        <link>http://localhost:4000/nlp/2019/07/01/explaining-analogies-explained.html</link>
        <guid isPermaLink="true">http://localhost:4000/nlp/2019/07/01/explaining-analogies-explained.html</guid>
        
        
        <category>NLP</category>
        
      </item>
    
  </channel>
</rss>
