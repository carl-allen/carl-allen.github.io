---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
title:  "About me..."
---

I am a [Laplace Postdoctoral Chair](https://data-ens.github.io/jobs/) in Data Science at École Normale Supérieure, Paris. Previously I was a postdoctoral fellow at [ETH Zurich](https://ai.ethz.ch/) after completing my PhD in 2021 at the [University of Edinburgh](https://www.ed.ac.uk/informatics), supervised by Profs [Tim 
Hospedales](https://homepages.inf.ed.ac.uk/thospeda/) and [Iain Murray](https://homepages.inf.ed.ac.uk/imurray2/bio.html).

My **main Research Interest** is in developing a mathematical understanding of how successful machine learning methods work (e.g. neural networks), with the aim of: 
(i) developing better performing, more interpretable and more reliable machine leraning algorithms; and perhaps
(ii) deepening our understanding of the underlying data, such as language, images, speech or DNA, in term of its latent structure and the mechanisms by which it can be learned; 
While my PhD focused on representing discrete objects, e.g. words, knowledge graph entities/relations and network/graph nodes, my current work considers more general latent variable/representation models.

My PhD ("[_Towards a Theoretical Understanding of Word and Relation Representation_](https://arxiv.org/pdf/2202.00486.pdf)" focused on developing a theoretical understanding of how words are _represented_, whether as word embeddings learned from huge text corpora (e.g. by word2vec or GloVe); or as entity 
embeddings learned from the facts (_subject, relation, object_) in a knowledge graph. 

During my PhD I spent 6 months on internship at [Samsung AI Centre, Cambridge](https://research.samsung.com/aicenter_cambridge) working at the interestection of representation learning and logical reasoning.

**Background**: I moved to Artificial Intelligence/Machine Learning research after some time working in [Project Finance](https://tenor.com/view/why-why-why-dee-pqperplqnes-gif-20613669).
I hold a BSc in Mathematics and Chemistry from the <a href="https://www.southampton.ac.uk/">University of Southampton</a>,
an MSc Mathematics and the Foundations of Computer Science (MFoCS) from the <a href="https://www.ox.ac.uk/">University of Oxford</a> and
MScs in Artificial Intelligence and Data Science from the <a href="https://www.ed.ac.uk/">University of Edinburgh</a>.

**Awards**: "Analogies Explained: Towards Understanding Word Embeddings" received Best Paper, Honourable Mention

## Publications

  <p><strong>A Probabilistic Model for Self-Supervised Learning</strong>
    <br />
  A Bizeul, B Schölkopf, <u>C Allen</u>;
  <em> under review</em>, 2024 <br />
  </p>

  <p><strong>Variational Classification: A Probabilistic Generalization of the Softmax Classifier</strong>
    <a href="https://arxiv.org/pdf/2305.10406">[arXiv]</a> <br />
  S Dhuliawala, M Sachan, <u>C Allen</u>;
  <em> TMLR</em>, 2024 <br />
  </p>

  <p><strong>Learning to Drop Out: An Adversarial Approach to Training Sequence VAEs</strong>
    <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/3ed57b293db0aab7cc30c44f45262348-Paper-Conference.pdf">[NeurIPS]</a> <br />
  Đ Miladinović, K Shridhar, K Jain, M Paulus, JM Buhmann, <u>C Allen</u>;
  <em> NeurIPS</em>, 2022 <br />
  </p>

  <p><strong>Adapters for Enhanced Modelling of Multilingual Knowledge and Text</strong>
    <a href="https://arxiv.org/pdf/2210.13617">[arXiv]</a> <br />
  Y Hou, W Jiao, M Liu, <u>C Allen</u>, Z Tu, M Sachan;
  <em> EMNLP</em>, 2022 <br />
  </p>

  <p><strong>Interpreting Knowledge Graph Relation Representation from Word Embeddings</strong>
    <a href="https://arxiv.org/pdf/1909.11611">[arXiv]</a> <br />
  <u>C Allen</u>*, I Balažević*, T Hospedales;
  <em> ICLR</em>, 2021 <br />
  </p>

  <p><strong>Multi-scale Attributed Embedding of Networks</strong>
    <a href="https://arxiv.org/abs/1909.13021">[arXiv]</a> <a href="https://github.com/benedekrozemberczki/MUSAE">[github]</a><br />
  B Rozemberczki, <u>C Allen</u>, R Sarkar;
  <em> Journal of Complex Networks </em>, 2021 <br />
  </p>


  <p><strong>What the Vec? Towards Probabilistically Grounded Embeddings</strong>
    <a href="https://arxiv.org/abs/1805.12164">[arXiv]</a><br />
  <u>C Allen</u>, I Balažević, T Hospedales;
  <em> NeurIPS</em>, 2019 <br />
  </p>

  <p><strong>Multi-relational Poincaré Graph Embeddings</strong>
    <a href="https://arxiv.org/abs/1905.09791">[arXiv]</a> <a href="https://github.com/ibalazevic/multirelational-poincare">[github]</a><br />
  I Balažević, <u>C Allen</u>, T Hospedales;
  <em> NeurIPS</em>, 2019 <br />
  </p>


  <p><strong>Analogies Explained: Towards Understanding Word Embeddings</strong>
    <a href="https://arxiv.org/abs/1901.09813">[arXiv]</a>
    <a href="https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html">[blog post]</a>
    <a href="/assets/Analogies_Explained_slides_ICML.pdf">[slides]</a><br />
  <u>C Allen</u>, T Hospedales;
  <em> ICML</em>, 2019 <strong>(honorable mention)</strong><br />
  </p>

  <p><strong>TuckER: Tensor Factorization for Knowledge Graph Completion</strong>
    <a href="https://arxiv.org/abs/1901.09590">[arXiv]</a> <a href="https://github.com/ibalazevic/TuckER">[github]</a><br />
  I Balažević, <u>C Allen</u>, T Hospedales;
  <em> EMNLP</em>, 2019 <strong>(oral)</strong> <br />
  </p>

  <p><strong>Hypernetwork Knowledge Graph Embeddings</strong>
    <a href="https://arxiv.org/abs/1808.07018">[arXiv]</a> <a href="https://github.com/ibalazevic/HypER">[github]</a><br />
  I Balažević, <u>C Allen</u>, T Hospedales;
  <em> ICANN</em>, 2019 <strong>(oral)</strong> <br />
  </p>

---
