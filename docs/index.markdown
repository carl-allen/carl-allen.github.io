---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
title:  "About me..."
---

I am a post doc in Machine Learning at the AI Centre, [ETH Zurich](https://ai.ethz.ch/). I completed my PhD in 2022 at the <a href="https://www.ed.ac.uk/informatics"> 
University of Edinburgh</a> as a member of the [Centre for Doctoral Training in Data Science](http://datascience.inf.ed.ac.uk/), supervised by Profs [Tim 
Hospedales](https://homepages.inf.ed.ac.uk/thospeda/) and [Iain Murray](https://homepages.inf.ed.ac.uk/imurray2/bio.html).
My PhD focused on developing a theoretical understanding of how words are _represented_: as word embeddings learned from huge text corpora (e.g. by word2vec or GloVe); or as entity 
embeddings learned from the facts of a knowledge graph. During my PhD I spent 6 months on internship at [Samsung AI Centre, Cambridge](https://research.samsung.com/aicenter_cambridge) looking into 
the interestection of representation learning and logical reasoning.

PhD thesis: "[_Towards a Theoretical Understanding of Word and Relation Representation_](https://arxiv.org/pdf/2202.00486.pdf)" 

My main research interest is in developing a theoretical/mathematical understanding of how successful machine learning methods work. The goal being to: 
(i) deepen our general understanding of the underlying latent structure of the data -- i.e. of language, images, speech, DNA, etc -- and of the often implicit mechanisms by which it can be learned; 
and 
from 
that 
(ii) develop improved, interpretable and controllable machine leraning algorithms.
Where my PhD focused on representing discrete objects (word embeddings, knowledge graph representation methods and network/graph embeddings), my current work abstracts this to consider more 
general latent variable/representation models.


## Background

I moved into Artificial Intelligence/Machine Learning research after some time working in [Project Finance](https://tenor.com/view/why-why-why-dee-pqperplqnes-gif-20613669).
I hold a BSc in Mathematics and Chemistry from the <a href="https://www.southampton.ac.uk/">University of Southampton</a>,
an MSc Mathematics and the Foundations of Computer Science (MFoCS) from the <a href="https://www.ox.ac.uk/">University of Oxford</a> and
MScs in Artificial Intelligence and Data Science from the <a href="https://www.ed.ac.uk/">University of Edinburgh</a>.


## Publications

  <p><strong>Interpreting Knowledge Graph Relation Representation from Word Embeddings</strong>
    <a href="https://arxiv.org/pdf/1909.11611">[arXiv]</a> <br />
  <u>C Allen</u>*, I Balažević*, T Hospedales;
  <em> ICLR</em>, 2021 <br />
  </p>

  <p><strong>Multi-scale Attributed Embedding of Networks</strong>
    <a href="https://arxiv.org/abs/1909.13021">[arXiv]</a> <a href="https://github.com/benedekrozemberczki/MUSAE">[github]</a><br />
  B Rozemberczki, <u>C Allen</u>, R Sarkar;
  <em> Journal of Complex Networks </em>, 2021 <br />
  </p>


  <p><strong>What the Vec? Towards Probabilistically Grounded Embeddings</strong>
    <a href="https://arxiv.org/abs/1805.12164">[arXiv]</a><br />
  <u>C Allen</u>, I Balažević, T Hospedales;
  <em> NeurIPS</em>, 2019 <br />
  </p>

  <p><strong>Multi-relational Poincaré Graph Embeddings</strong>
    <a href="https://arxiv.org/abs/1905.09791">[arXiv]</a> <a href="https://github.com/ibalazevic/multirelational-poincare">[github]</a><br />
  I Balažević, <u>C Allen</u>, T Hospedales;
  <em> NeurIPS</em>, 2019 <br />
  </p>


  <p><strong>Analogies Explained: Towards Understanding Word Embeddings</strong>
    <a href="https://arxiv.org/abs/1901.09813">[arXiv]</a>
    <a href="https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html">[blog post]</a>
    <a href="/assets/Analogies_Explained_slides_ICML.pdf">[slides]</a><br />
  <u>C Allen</u>, T Hospedales;
  <em> ICML</em>, 2019 <strong>(honorable mention)</strong><br />
  </p>

  <p><strong>TuckER: Tensor Factorization for Knowledge Graph Completion</strong>
    <a href="https://arxiv.org/abs/1901.09590">[arXiv]</a> <a href="https://github.com/ibalazevic/TuckER">[github]</a><br />
  I Balažević, <u>C Allen</u>, T Hospedales;
  <em> EMNLP</em>, 2019 <strong>(oral)</strong> <br />
  </p>

  <p><strong>Hypernetwork Knowledge Graph Embeddings</strong>
    <a href="https://arxiv.org/abs/1808.07018">[arXiv]</a> <a href="https://github.com/ibalazevic/HypER">[github]</a><br />
  I Balažević, <u>C Allen</u>, T Hospedales;
  <em> ICANN</em>, 2019 <strong>(oral)</strong> <br />
  </p>

---
