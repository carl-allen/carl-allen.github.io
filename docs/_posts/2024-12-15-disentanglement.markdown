---
layout:     post
title:      "Disentangling Disentanglement: How VAEs Learn Independent Components"
date:       2024-12-15 21:39:00 +0100
author:     Carl Allen
paper-link: https://arxiv.org/pdf/2410.22559
link-text:  "[arXiv]"
categories: Theory

---

{% include_relative _includes/head.html %}


<table>
  <tr>
    <th>This post summarises <a href="[url](https://arxiv.org/pdf/2410.22559)">Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components (Allen, 2024)</a>, which explains <b>why disentangelement arises in generative latent variable models</b>. </th>
  </tr>
</table>

**Disentanglement** is an intriguing phenomenon observed in generative latent variable models, particularly [_Variational Autoencoders_ (VAEs)][VAE] (our focus) and [Generative Adversarial Networks (GANs)](https://www.youtube.com/watch?v=DbQNKdtoqUw). Disentanglement has not been rigorously defined but refers to when semantically meaningful factors of the data map to distinct dimensions in latent space. This allows, for example, images to be generated that vary in a specific feature (e.g. hair style, orientation) by changing a *single latent dimension*. 

<p align="center">
        <img src="/assets/disentanglement/faces2.png" 
                alt="faces" 
                width="420" 
                height="70" 
                style="display: block; margin: 0 auto"  />
        <img src="/assets/disentanglement/numbers.png" 
                alt="numbers" 
                width="420" 
                height="70" 
                style="display: block; margin: 0 auto"  />
        <img src="/assets/disentanglement/frogs.png" 
                alt="frogs" 
                width="420" 
                height="70" 
                style="display: block; margin: 0 auto"  />
Illustrating disentangelement: each row of images varies by a single semantic factor, e.g. hair style, line thickness, orientation. When disentangled, each row of images can be generated by varying a single dimension of the latent variable or, equivalently, by moving in an "axis-aligned" direction in latent space. (Images from <a href="[url](https://arxiv.org/pdf/2002.03754)">Voynov & Babenko (ICML, 2020))</a>
</p>

While disentanglement is often associated with certain models whose popularity may ebb and flow, e.g. VAE, $\beta$-VAE, GANs, we show that the phenomenon itself **relates to the latent structure of the data** and is more fundamental than any particular model that may expose it.

**Motivation**: Disentanglement is particularly intriguing because models are not intentially designed to achieve it, and it is observed even in settings where it is deemed impossible.[^locatello][^khemakem] Thus understanding disentanglement we offer new insights into how and what models learn and revise current thinking on identifiability. More broadly, the ability to separate independent aspects of the data is relevant to many areas of machine learning and, by enabling generative factors to be teased apart, may offer fundamental insights into the data itself. 

**Approach**: Recent works[^rolinek][^kumarpoole] suggest that disentanglement in VAEs results from commonly used *diagonal posterior covariance matrices* promoting *column-orthgonality in the decoder's Jacobian matrix*. Building on this, [Allen (2024)][paper]: (A) clarifies the connection between diagonal covariance and column-orthogonality and (B) explains how that leads to disentanglement; ultimately showing that **disentanglement equates to factorising the data distribution into *statistically independent components***.

<table>
  <tr>
    <th>$$
        \begin{equation}
        \text{diag. posterior covariance} 
            \quad  \overset{A}{\Rightarrow}\quad   \text{column-orthog. Jacobian} 
            \quad \overset{B}{\Rightarrow}\quad    \text{disentanglement}
        \end{equation}
        $$
 </th>
  </tr>
</table>

---
---
<br>

**Problem set-up**: As in a VAE (or GAN), we assume that data $$x\in\mathcal{X} \subseteq\mathbb{R}^n$$ are generated by a latent variable model: independently sampled latent variables $$z\in\mathcal{Z} =\mathbb{R}^m$$, $$p(z) = \prod_ip(z_i)$$, are mapped <abbr title="A deterministic GAN generator can be thought of a limiting case as Var[x|z] tending to 0.">stochastically to data space $$x\sim p(x\mid z)$$</abbr>. The latent prior is commonly assumed to be isotropically Gaussian $$p_\theta(z_i) = \mathcal{N}(z_i,0,1)$$. We also assume $$p(x\mid z)$$ is Gaussian.

**Notation**: un-subscripted probability distributions denote the ground truth and subscripts denote a model of that distribution.

**TL;DR**: 
* Disentanglement provably occurs in the _linear_ case ($$p_\theta(x\mid z) = \mathcal{N}(x; Dz,\sigma^2I),\ D\in\mathbb{R}^{n\times m})$$ where analytic solutions are given by [probabilistic PCA (PPCA)][PPCA]. However, a VAE with diagonal posterior covariance finds a subset of PPCA's solutions in which latent dimensions $$z_i$$ *map to independent factors of variation* in the data distribution (disentanglement).
* Surprisingly, the same rationale behind the linear case (described below) extends to non-linear VAEs where diagonal posterior covariances encourage columns of the decoder's Jacobian to be orthogonal, meaning that independent latent variables pass through the decoder separably and emerge in $$\mathcal{X}$$ over statistically independent sub-manifolds of the decoder-defined manifold.
* That is, independent latent variables over which $p(z)$ factorises are mapped by the decoder to independent components over which the manifold distribution factorises.
* Since a VAE's objective is maximised if the model distribution matches that of the data, if data are generated under that model - and the ground truth distribution therefore factorises into statistically independent factors - then independent components of the model corresponding to distinct latent variables align with independent components of the data (disentanglement).

---
---
<br>

<table>
  <tr>
    <th>$$
    \begin{equation}
    \text{diag. posterior covariance} 
        \quad  \overset{A}{\Rightarrow}\quad   \text{column-orthog. Jacobian} 
        \color{lightgray}{\quad \overset{B}{\Rightarrow}\quad    \text{disentanglement}}
    \end{equation}
    $$
 </th>
  </tr>
</table>

### A: From Diagonal Covariance to Jacobian Orthogonality

The VAE fits a latent variable model $$p_\theta(x) =\int_z p_\theta(x\mid z)p(z)$$ to  the data distribution $$p(x)$$ by maximising the Evidence Lower Bound (ELBO),[^ELBO]

$$\ell(\theta, \phi) \quad =\quad \int p(x) \int q_\phi(z\mid x) 
\ \{\ \log p_\theta(x\mid z) \,-\, \beta \log \tfrac{q_\phi(z\mid x)}{p(z)} \ \}\ dz dx\ ,$$

where the ELBO has $$\beta=1$$ but [$$\beta>1$$ is found to improve disentanglement][betaVAE]. We assume common VAE assumptions:
* $$p_\theta(x\mid z) =\mathcal{N}(x;\,d(x),\sigma^2)\quad$$ with *decoder*  $$d$$ and fixed variance $$\sigma^2$$;
* $$q_\phi(z\mid x)=\mathcal{N}(z;\,e(x),\Sigma_x)\quad$$ with *encoder* $$e$$ and learned variance $$\Sigma_x$$; and
* $$p(z)\quad\ \ \ =\mathcal{N}(z;\,0,I)\quad$$ where $$z_i$$ are *independent* with $$p(z_i)=\mathcal{N}(z_i;0,1)$$.

Note:
* the VAE decoder $$d$$ maps latent variables $$z\in\mathcal{Z}$$ to means $$\mu_z=\mathbb{E}[x\mid z]\in \mathcal{X}$$
* if $$J_z$$ denotes $$d$$'s Jacobian evaluated at $$z$$, then $$J_{i,j} = \tfrac{\partial d(z)_i}{\partial z_j}$$ defines how a perturbation in the latent space (in direction $$z_j$$) translates to variation in the data space (in direction $$x_i$$).

The optimal $$\Sigma_x$$ is defined by the Hessian of $$\log p_\theta(x\mid z)$$ ([Opper & Archambeau](http://www0.cs.ucl.ac.uk/staff/c.archambeau/publ/neco_mo09_web.pdf)), meaning that if the decoder's second derivatives are small almost everywhere, e.g. as in a Relu network [(see Abhishek & Kumar, 2020)](https://arxiv.org/pdf/2002.00041), we have:

$$
\begin{equation}
  \Sigma_x 
    \ \ \overset{O\&A}{=}\ \ I - \mathbb{E}_{q(z\mid x)}[\tfrac{\partial^2\log p_\theta(x\mid z)}{\partial z_i\partial z_j}]
    \ \ \approx\ \ I + \tfrac{1}{\beta\sigma^2}J_z^\top J_z\ ,
  \tag{1}\label{eq:one}
\end{equation}
$$

and the ELBO is maximised for diagonal $$\Sigma_x$$ when **columns of $$J_z$$ are orthogonal** (as suggested previously[^rolinek][^kumarpoole]). By implication, if $$J_z=U_zS_zV_z^\top$$ is the singular value decomposition (SVD), then $$V_z=I,\ \forall z$$ and variation in a single latent component $$z_i$$ corresponds to a variation in data space in direction $$u_i$$, the $$i^{th}$$ left singular vector of $$J_z$$ (column $$i$$ of $$U_z$$) with no affect in any orthogonal direction $$u_{j\ne i}$$.

> **Take-away**: the ELBO is maximised if approximate posterior covariances match true posterior covariances, which can be expressed in terms of derivatives of $$p_\theta(x\mid z)$$. This does not mean the Hessian is necessarily orthogonal, but if such solutions exists then the VAE tries to find them.
<!-- (hinting towards learning independent factors). -->

---
---

<table>
  <tr>
    <th>$$
      \begin{equation}
      {\color{lightgray}{\text{diag. posterior covariance} 
          \quad  \overset{A}{\Rightarrow}}}\quad   \text{column-orthog. Jacobian}
          \quad \overset{B}{\Rightarrow}\quad    \text{disentanglement}
      \end{equation}
      $$
 </th>
  </tr>
</table>


### B: From Orthogonality to Statistical Independence

Having seen that diagonal covariances promote column-orthogonality in the decoder Jacobian, the question is how this geometric proprety leads to the statistical property of disentanglement. To understand it, we consider the *push-forward* distribution defined by the decoder, which is supported over a manifold $$\mathcal{M}_d\subseteq\mathcal{X}$$. 

> A **push-forward distribution** describes the probability distribution of the output of a deterministic function given an input distribution. A VAE decoder latent samples of the prior $$p(z)$$ to the data space, defining a push-forward distribution  over $$\mathcal{M}_d$$. 

#### Linear Case

<img src="/assets/disentanglement/linear.png" 
        alt="linear2" 
        width="440" 
        height="190" 
        style="display: block; margin: 0 auto"  />


For intuition, we  consider the linear case $$x=d(z)=Dz$$,  $$D\in\mathbb{R}^{n\times m}$$, the model considered in [Probabilistic PCA (PPCA)][PPCA], which has a tractible MLE solution and known optimal posterior

$$
\begin{equation}
  p_\theta(z\mid x) = \mathcal{N}(z;\, \tfrac{1}{\sigma^2}M D^\top x,\, M) \quad\quad\quad M = (I + \tfrac{1}{\sigma^2}D^\top D)^{-1}
  \tag{2}\label{eq:two}
\end{equation}
$$

This can be seen as a special case of \eqref{eq:one}, thus the ELBO is maximised if $$\Sigma_x=M,\ \forall x\in\mathcal{X}$$, and using diagonal posteriors $$\Sigma_x$$ again implies $$V=I$$ (for SVD $$D=USV^\top$$). 

For a given point $$z^*\in \mathcal{Z}$$: 
* we define lines $$\mathcal{Z^{(i)}}\subset\mathcal{Z}$$ passing through $$z^*$$ parallel to each standard basis vector $$e_i$$, and their images under $$D$$, $$\mathcal{M}_D^{(i)}\subset\mathcal{M_d}$$ (lines following $$D$$'s left singular vectors); and
* consider $$u=U^\top x$$  ($$x$$ in the basis defined by columns of $$U$$), noting that: $$\tfrac{\partial u_i}{\partial z_j} =\{s_i \text{ if }i=j; 0 \text{ o/w}\}$$ (since $$\tfrac{dx}{dz} = \tfrac{dx}{du}\tfrac{du}{dz} = US$$ and $$\tfrac{dx}{du} = U$$).

The point here is to identify how independent dimensions $$z_i\in\mathcal{Z}$$ "flow" under the decoder. Indeed, by considering $$x$$ in the "$$U$$-basis", independent $$z_i$$ become independent components $$u_i$$, since it can be shown that:
1. $$\{u_i\}_i$$ are observations of *independent* random variables;
2. the push-forward of $$d$$ restricted to $$\mathcal{Z^{(i)}}$$ has density $$p(u_i) = s_i^{-1}p(z_i)$$ over $$\mathcal{M}_D^{(i)}$$;
3. the full push-forward satisfies $$p(Dz) = \mid D\mid ^{-1}p(z) = \prod_i s_i^{-1}p(z_i) = \prod _ip(u_i)$$.

Altogether, this shows that the push-forward distribution generated by the decoder factorises as a product of independent univariate distributions ($$p(u_i)$$), each corresponding to a distinct latent dimension ($$z_i$$). Thus, if the data follows the assumed generateive process and itself factorises with unique factors (determined by ground truth $$s_i$$), then the ELBO is maximised when the independent components of the data and model distributions align and p(x) is **disentangled** as a product of *independent components* aligned with each latent dimension.

This is not so surprising in the linear case, since we know from the outset that the push-forward distribution is Gaussian and so necessarily factorises as a product of univariate Gaussians. However, we did not use that fact or rely on the linearity of $$d$$ at any step.

<!-- From $$J_z =\tfrac{d x}{d z}=US, \tfrac{d x}{d u} = U$$, we have $$\tfrac{d u}{d z} =S$$, so $$\tfrac{\partial u_i}{\partial z_j} = \{s_i$$ if $$i= j$$, otherwise $$0\}$$ and each $$u_i$$ depends on a distinct independent r.v. so are *independent*.
2. restricting $$D$$ to $$z_i\in \mathcal{Z}^{(i)}$$ and so $$u_i\in\mathcal{M_D^{(i)}}$$, $$p(u_i)= s_i^{-1}p(z_i)$$ (defined over $$\mathcal{M_D^{(i)}}$$)
 -->


#### Non-linear Case with Diagonal Jacobian
<img src="/assets/disentanglement/non_linear.png" 
        alt="non_linear2" 
        width="420" 
        height="200" 
        style="display: block; margin: 0 auto"  />



We now take an analogous approach for the general VAE ($$x=d(z)$$, $$d\in\mathcal{C}^2$$) with column-orthogonal decoder Jacobian. Notably, the Jacobian and its factors, $$J_z=U_zS_zV_z^\top$$, now depend on $$z$$, although column-orthgonality implies $$V_z=I,\ \forall z\in \mathcal{Z}$$ and $$U_z$$, $$S_z$$ are continuous w.r.t. $$z$$ since $$d\in\mathcal{C}^2$$. 

As previously, for a given point $$z^*\in \mathcal{Z}$$, we define lines $$\mathcal{Z^{(i)}}\subset\mathcal{Z}$$ passing through $$z^*$$ parallel to the standard basis (axis-aligned), and their images under $$d$$, $$\mathcal{M}_d^{(i)}\subset\mathcal{M_d}$$, which are potentially curved sub-manifolds, following (local) left singular vectors of $$D$$.

We again consider $$x$$ in the (local) basis defined by columns of $$U$$ as $$u=U^\top x$$ and still have $$\tfrac{\partial u_i}{\partial z_j} =\{s_i \text{ if }i=j; 0 \text{ o/w}\}$$.

As previously, we claim that independent dimensions $$z_i\in\mathcal{Z}$$ flow under the decoder to become independent components $$u_i$$ since:
1. $$\{u_i\}_i$$ are observations of *independent* random variables;
2. the push-forward of $$d$$ restricted to $$\mathcal{Z^{(i)}}$$ has density $$p(u_i) = s_i^{-1}p(z_i)$$ over $$\mathcal{M}_d^{(i)}$$;
3. the full push-forward satisfies $$p(d(z)) = \mid J_z\mid ^{-1}p(z) = \prod_i s_i^{-1}p(z_i) = \prod _ip(u_i)$$.

Thus, following the same argument as in the linear case, the distribution over the decoder manifold factorises as a product of independent univariate push-foward distributions ($$p(u_i)$$), each corresponding to a distinct latent dimension ($$z_i$$). Again, if the true data distribution follows this generative process and so factorises and those factors are unique, then the ELBO is maximised when components of the model fit to those of the data and $$p(x)$$ is **disentangled** as a product of *independent components* aligned with each latent dimension. Each component is supported on a sub-manifold orthogonal to the others, capturing the variations along a single latent dimension.

We recommend reading [the full paper][paper] for further details, such as:
* consideration of whether orthogonality is strictly _necessary_ for disentanglement (the argument above shows it is _sufficient_)
* _identifiability_ of the model, i.e. up to what symmetries can the VAE identify the ground truth generative factors
* the role of parameter $\beta$ in a [$\beta$-VAE][betaVAE]
  * spoiler: $$\beta$$ is proportional to Var$$_\theta[x\mid z]$$ where $$p_\theta(x\mid z)$$ is of exponential family form (generalising $\sigma^2$ of a Gaussian-VAE).
  * $\beta$ determines how close data points need to be (in Euclidean norm) to be deemed similar and their representations merge.

<!-- 
**Interpreting β in β-VAEs**

β-VAEs introduce a hyperparameter β that scales the KL divergence term in the ELBO. Empirical studies have shown that increasing β enhances disentanglement, often at the cost of reconstruction quality. The sources offer a novel interpretation of β, viewing it as a factor scaling the variance of the likelihood distribution.

Increasing β corresponds to increasing the variance of the likelihood, essentially making the model more uncertain about its reconstructions. This increased uncertainty leads to greater overlap between the posterior distributions of nearby data points. As the ELBO encourages Jacobian orthogonality in expectation over the posterior distributions, larger overlap implies that orthogonality constraints apply over a broader region in the latent space, promoting stronger disentanglement.

Conversely, decreasing β reduces the likelihood variance, mitigating the issue of "posterior collapse". This phenomenon occurs when the likelihood becomes overly expressive, directly modeling the data distribution and rendering the latent variables redundant. By reducing the likelihood variance, β < 1 constrains the model's flexibility, preventing it from learning a trivial solution. -->

### Conclusion

[Allen (2024)][paper] provides a compelling theoretical argument for the link between diagonal posterior covariance, Jacobian orthogonality, and disentanglement in VAEs. The analysis clarifies how a simple design choice, motivated by computational efficiency, leads to the learning of statistically independent data components. We hope that understanding this connection gives new insight into how VAEs work, how understanding of linear models may extend surprisingly well to non-linear models and may lead to new training algorithms that can more reliably achieve disentangelement.



[paper]: https://arxiv.org/pdf/2410.22559
[VAE]: https://arxiv.org/pdf/1312.6114v11
[betaVAE]: https://openreview.net/forum?id=Sy2fzU9gl
[PPCA]: https://academic.oup.com/jrsssb/article-abstract/61/3/611/7083217

[^rolinek]: [Variational Autoencoders Pursue PCA Directions (by Accident); Rolinek et al. (CVPR, 2019)](https://arxiv.org/pdf/1812.06775)
[^kumarpoole]: [On Implicit Regularization in β-VAEs; Kumar \& Poole (ICML, 2020)](https://arxiv.org/pdf/2002.00041)
[^locatello]: [Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations; Locatello et al. (ICML, 2019)](https://arxiv.org/pdf/1811.12359)
[^khemakem]: [Variational Autoencoders and Nonlinear ICA: A Unifying Framework; Khemakhem et al. (AIStats, 2020)](https://proceedings.mlr.press/v108/khemakhem20a/khemakhem20a.pdf)
[^ELBO]: Maximising the ELBO = _maximum-likelihood$$^{++}$$_:  Maximising the likelihood $$\int p(x)\log p_\theta(x)$$ minimises the KL divergence between the data and model distributions, but this is often intractible for a latent variable model. Maximising the ELBO minimises the KL divergences between $$p(x)q_\phi(z\mid x)$$ *and* $$p_\theta(x)p_\theta(z\mid x)\doteq p_\theta(x\mid z)p(z)$$, aligning two models of the joint distribution.
