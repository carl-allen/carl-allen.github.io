---
layout:     post
title:      "Disentangling Disentanglement: How VAEs Learn Independent Components"
date:       2024-12-15 21:39:00 +0100
author:     Carl Allen
paper-link: https://arxiv.org/pdf/2410.22559
link-text:  "[arXiv]"
categories: Theory

---

{% include_relative _includes/head.html %}

<table>
  <tr>
    <th>This post summarises <a href="[url](https://arxiv.org/pdf/2410.22559)">Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components (Allen, 2024)</a>, which explains <b>why disentanglement arises in generative latent variable models</b>. </th>
  </tr>
</table>

**Disentanglement** is an intriguing phenomenon observed in generative latent variable models, such as [_Variational Autoencoders_ (VAEs)][VAE] (our focus), [Generative Adversarial Networks (GANs)](https://arxiv.org/pdf/1406.2661) and [latent Diffusion models](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf). Although disentanglement has not been rigorously defined, it refers to when semantically meaningful factors of the data map to distinct dimensions in latent space. This allows, for example, images to be generated that vary in a specific feature (e.g. hair style, orientation) by changing a *single latent dimension* (see Figure 1 and [this video that nicely describes disentanglement in GANs](https://www.youtube.com/watch?v=DbQNKdtoqUw)). 

While disentanglement is often associated with certain models whose popularity may ebb and flow (e.g. VAEs, $\beta$-VAEs, GANs), we show that the phenomenon itself **relates to the latent structure of the data** and is more fundamental than any model that may expose it.

<br>
<style>
#left {
   width: 75%;
    float:left;
}

#rightAside {
    width: 25%;
    float:right;
}
</style>

<p align="center">
  <div id="left">
    <img src="/assets/disentanglement/faces2.png" 
                    alt="faces" 
                    width="490" 
                    height="70"/>
            <img src="/assets/disentanglement/numbers.png" 
                    alt="numbers" 
                    width="493" 
                    height="70"/>
            <img src="/assets/disentanglement/frogs.png" 
                    alt="frogs" 
                    width="490" 
                    height="70"/>
  </div>
  <div id="rightAside">
            <img src="/assets/disentanglement/diffusion_church.png" 
                  alt="frogs" 
                  width="180" 
                  height="210"/>
  </div>
</p>
<p align="center">
    <small>Figure 1. Illustrating disentanglement: <emph>(left)</emph> Each row of images varies by a single semantic feature, e.g. hair style orientation. A feature is "disentangled" if such images can be generated by varying a single latent dimension, i.e. distinct latent variables control distinct semantic features. This equates to traversing the latent space parallel to standard basis vectors or in "axis-aligned" directions. <emph>(right)</emph> Using disentanglement to change the style of an image in diffusion models. (Images from <a href="[url](https://arxiv.org/pdf/2002.03754)">Voynov & Babenko (2020)</a> and <a href="[url](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf)">Wu et al. (2023))</a></small>
</p>


$$\bullet$$ **Motivation**: Disentanglement is particularly intriguing because models such as VAEs are not intentionally designed to achieve it, and it occurs even where it is deemed impossible.[^locatello]$$^,$$[^khemakem] Thus understanding disentanglement may offer new insights into how and what these models learn and revise our understanding on what is learnable. More broadly, the ability to separate out independent aspects of the data is relevant to many areas of machine learning and, by teasing apart its generative factors, may offer fundamental insights into the data itself. 

Machine learning has made incredible breakthroughs in recent years but our theoretical understanding lags notably behind, which may have serious implications as models transfer from academic labs to the public sphere. Thus, a fundamental motivation for this work is to unravel some of the mystery of machine learning, hence our emphasis that disentanglement is not a model feature per se, more a property of the data that a model may or may not expose and of more general interest to understand.

$$\bullet$$ **Approach**: Recent works suggest that disentanglement arises in VAEs because commonly used *diagonal posterior covariance matrices* promote *column-orthogonality in the decoder's Jacobian matrix*. Optimal posterior covariances are shown to relate (approximately) to the decoder Jacobian[^rolinek]$$^,$$[^kumarpoole]; the decoder Jacobian is shown empirically to be more orthogonal in VAEs with diagonal covariance than those with full covariance or general auto-encoders;[^rolinek] and directly imposing such orthogonality is shown to induce disentanglement.[^kumarpoole] Building on this, [Allen (2024)][paper]: (A) clarifies the connection between diagonal covariance and column-orthogonality and (B) explains how that leads to disentanglement, ultimately **defining disentanglement as factorising the data distribution into *statistically independent components***.



<table>
  <tr>
    <th>$$
        \begin{equation}
        \text{diag. posterior covariance} 
            \quad  \overset{A}{\Rightarrow}\quad   \text{column-orthog. Jacobian} 
            \quad \overset{B}{\Rightarrow}\quad    \text{disentanglement}
        \end{equation}
        $$
 </th>
  </tr>
</table>


$$\bullet$$ **Problem set-up**: As in a VAE (or GAN), we assume that data $$x\in\mathcal{X} \subseteq\mathbb{R}^n$$ are generated by a latent variable model: independently sampled latent variables $$z\in\mathcal{Z} =\mathbb{R}^m$$, $$p(z) = \prod_ip(z_i)$$, are mapped <abbr title="A deterministic GAN generator can be thought of the limiting case as Var[x|z] tends to 0.">stochastically to data space $$x\sim p(x\mid z)$$</abbr>. The latent prior is commonly assumed to be isotropically Gaussian $$p_\theta(z_i) = \mathcal{N}(z_i,0,1)$$. We also assume $$p(x\mid z)$$ is Gaussian.

[Notation: unsubscripted probability distributions denote the ground truth; subscripts denote a modelled distribution.]

---
---

**TL;DR**: 
* Disentanglement provably occurs in the _linear_ case ($$p_\theta(x\mid z) = \mathcal{N}(x; Dz,\sigma^2I),\ D\in\mathbb{R}^{n\times m})$$ where analytic solutions are given by [probabilistic PCA (PPCA)][PPCA]. However, a VAE with diagonal posterior covariance finds a subset of PPCA's solutions in which latent dimensions $$z_i$$ *map to independent factors of variation* in the data distribution (disentanglement).
* Surprisingly, the same rationale behind the linear case (described below) extends to non-linear VAEs where diagonal posterior covariances encourage columns of the decoder's Jacobian to be orthogonal, meaning that independent latent variables pass through the decoder separably and emerge in $$\mathcal{X}$$ over statistically independent sub-manifolds of the decoder-defined manifold.
* That is, independent latent variables over which $p(z)$ factorises are mapped by the decoder to independent components over which the manifold distribution factorises.
* Since a VAE's objective is maximised if the model distribution matches that of the data, if data are generated under that model - and the ground truth distribution therefore factorises into statistically independent factors - then independent components of the model corresponding to distinct latent variables align with independent components of the data (disentanglement).

---
---
<br>

<table>
  <tr>
    <th>$$
    \begin{equation}
    \text{diag. posterior covariance} 
        \quad  \overset{A}{\Rightarrow}\quad   \text{column-orthog. Jacobian} 
        \color{lightgray}{\quad \overset{B}{\Rightarrow}\quad    \text{disentanglement}}
    \end{equation}
    $$
 </th>
  </tr>
</table>

### (A) From Diagonal Covariance to Jacobian Orthogonality

The VAE fits a latent variable model $$p_\theta(x) =\int_z p_\theta(x\mid z)p(z)$$ to  the data distribution $$p(x)$$ by maximising the Evidence Lower Bound (ELBO),

$$\ell(\theta, \phi) \quad =\quad \int p(x) \int q_\phi(z\mid x) 
\ \{\ \log p_\theta(x\mid z) \,-\, \beta \log \tfrac{q_\phi(z\mid x)}{p(z)} \ \}\ dz dx\ ,$$

where the ELBO has $$\beta=1$$ but [$$\beta>1$$ is found to improve disentanglement][betaVAE]. We assume common VAE assumptions:
* $$p_\theta(x\mid z) =\mathcal{N}(x;\,d(x),\sigma^2)\quad$$ with *decoder* $$d\in\mathcal{C}^2$$ (injective) and fixed variance $$\sigma^2$$;
* $$q_\phi(z\mid x)=\mathcal{N}(z;\,e(x),\Sigma_x)\quad$$ with *encoder* $$e$$ and learned variance $$\Sigma_x$$; and
* $$p(z)\quad\ \ \ =\mathcal{N}(z;\,0,I)\quad$$ where $$z_i$$ are *independent* with $$p(z_i)=\mathcal{N}(z_i;0,1)$$.

> **Maximising the ELBO = _maximum-likelihood$$^{++}$$_**:  Maximising the likelihood $$\int p(x)\log p_\theta(x)$$ minimises the KL divergence between the data and model distributions, but this is often intractable for a latent variable model. Maximising the ELBO minimises the KL divergences between $$p(x)q_\phi(z\mid x)$$ *and* $$p_\theta(x)p_\theta(z\mid x)\doteq p_\theta(x\mid z)p(z)$$, aligning two models of the joint distribution.
> Note:
> * the VAE decoder $$d$$ maps latent variables $$z\in\mathcal{Z}$$ to means $$\mu_z=\mathbb{E}[x\mid z]\in \mathcal{X}$$
> * if $$J_z$$ denotes $$d$$'s Jacobian evaluated at $$z$$, then $$J_{i,j} = \tfrac{\partial d(z)_i}{\partial z_j}$$ defines how a perturbation in the latent space (in direction $$z_j$$) translates to variation in the data space (in direction $$x_i$$).

The optimal $$\Sigma_x$$ is defined by the Hessian of $$\log p_\theta(x\mid z)$$ ([Opper & Archambeau](http://www0.cs.ucl.ac.uk/staff/c.archambeau/publ/neco_mo09_web.pdf)), meaning that if the decoder's second derivatives are small almost everywhere, e.g. as in a ReLU network [(see Abhishek & Kumar, 2020)](https://arxiv.org/pdf/2002.00041), we have:

$$
\begin{equation}
  \Sigma_x 
    \ \ \overset{O\&A}{=}\ \ I - \mathbb{E}_{q(z\mid x)}[\tfrac{\partial^2\log p_\theta(x\mid z)}{\partial z_i\partial z_j}]
    \ \ \approx\ \ I + \tfrac{1}{\beta\sigma^2}J_z^\top J_z\ ,
  \tag{1}\label{eq:one}
\end{equation}
$$

and the ELBO is maximised for diagonal $$\Sigma_x$$ when **columns of $$J_z$$ are orthogonal** (as suggested previously[^rolinek]$$^,$$[^kumarpoole]). If $$J_z=U_zS_zV_z^\top$$ is the singular value decomposition (SVD), that implies $$V_z=I,\ \forall z$$ and so variation in a latent component $$z_i$$ corresponds to a variation in data space in direction $$\mathbf{u}_i$$, the $$i^{th}$$ left singular vector of $$J_z$$ (column $$i$$ of $$U_z$$) with no effect in any orthogonal direction $$\mathbf{u}_{j\ne i}$$.


> **Take-away**: the ELBO is maximised if approximate posterior covariances match true posterior covariances, which can be expressed in terms of derivatives of $$p_\theta(x\mid z)$$. Diagonal posterior covariance does not imply that the Hessian is necessarily orthogonal, but if multiple solutions exist, the VAE prefers those where the Hessian is diagonal and so columns of the Jacobian are orthogonal.
<!-- (hinting towards learning independent factors). -->

---
---
<br>

<table>
  <tr>
    <th>$$
      \begin{equation}
      {\color{lightgray}{\text{diag. posterior covariance} 
          \quad  \overset{A}{\Rightarrow}}}\quad   \text{column-orthog. Jacobian}
          \quad \overset{B}{\Rightarrow}\quad    \text{disentanglement}
      \end{equation}
      $$
 </th>
  </tr>
</table>


### (B) From Orthogonality to Statistical Independence

Understanding that diagonal posterior covariance promotes column-orthogonality in the decoder Jacobian, the question then is how does that _geometric_ property leads to the _statistical_ property of disentanglement. For that we consider the *push-forward* distribution defined by the decoder, supported over a $$m$$-dimensional manifold $$\mathcal{M}_d\subseteq\mathcal{X}$$. 

> A **push-forward distribution** describes the probability distribution of the output of a deterministic function given an input distribution. A VAE decoder maps samples of the latent prior $$p(z)$$ to the data space, where it defines a manifold $$\mathcal{M}_d$$ and push-forward distribution over it. 

---

#### Linear Case

For intuition, we first consider the linear case $$x=d(z)=Dz$$,  $$D\in\mathbb{R}^{n\times m}$$, the model considered in [Probabilistic PCA (PPCA)][PPCA], which has a tractable MLE solution and known optimal posterior

$$
\begin{equation}
  p_\theta(z\mid x) = \mathcal{N}(z;\, \tfrac{1}{\sigma^2}M D^\top x,\, M) \quad\quad\quad M = (I + \tfrac{1}{\sigma^2}D^\top D)^{-1}
  \tag{2}\label{eq:two}
\end{equation}
$$

This can be seen as a special case of \eqref{eq:one}, thus the ELBO is maximised if $$\Sigma_x=M,\ \forall x\in\mathcal{X}$$, and using diagonal posteriors $$\Sigma_x$$ again implies $$V=I$$ (for SVD $$D=USV^\top$$). 

<p align="center">
  <img src="/assets/disentanglement/linear.png" 
          alt="linear2" 
          width="440" 
          height="190" 
          style="display: block; margin: 0 auto"  />
  <small>Illustration of linear decoder $d:\mathcal{X}\to\mathcal{Z}$. The latent Gaussian prior is mapped to a Gaussian distribution in the data space over a linear manifold. </small>
</p>


We aim to understand how independent dimensions $$z_i\in\mathcal{Z}$$, and the probability density over them, pass through the decoder. For a given point $$z^*\in \mathcal{Z}$$, we: 
* define lines $$\mathcal{Z^{(i)}}\subset\mathcal{Z}$$ passing through $$z^*$$ parallel to each standard basis vector $$e_i$$ (blue dashed lines), and their images under $$d$$, $$\mathcal{M}_D^{(i)}\subset\mathcal{M_d}$$ (red dashed lines that follow $$D$$'s left singular vectors $$\mathbf{u}_i$$); and
* consider $$x^{(U)}=U^\top x=U^\top Dz^*$$  (i.e. $x$ in the basis defined by columns of $$U$$), noting that: $$\tfrac{\partial x^{(U)}_i}{\partial z_j} =\{s_i \text{ if }i=j; 0 \text{ o/w}\}$$ (since $$\tfrac{dx}{dz} = \tfrac{dx}{dx^{(U)}}\tfrac{dx^{(U)}}{dz} = US$$ and $$\tfrac{dx}{dx^{(U)}} = U$$).

By considering $$x$$ from this perspective (the "$$U$$-basis") we see that independent $$z_i$$ map to independent components $$x^{(U)}_i$$ since:
1. each $$x^{(U)}_i$$ varies only with a distinct $z_i$ (from $$\tfrac{\partial x^{(U)}_i}{\partial z_j}$$ above) and so are *independent*;
2. the push-forward of $$d$$ restricted to $$\mathcal{Z^{(i)}}$$ has density $$p(x^{(U)}_i) = s_i^{-1}p(z_i)$$ over $$\mathcal{M}_D^{(i)}$$; and
3. the full push-forward satisfies $$p(Dz) = \mid\!D\!\mid ^{-1}p(z) = \prod_i s_i^{-1}p(z_i) = \prod _ip(x^{(U)}_i)$$.

This shows that the push-forward distribution defined by the decoder factorises as a product of independent univariate distributions ($$p(x^{(U)}_i)$$), each corresponding to a distinct latent dimension $$z_i$$. Thus, if the data follows that generative process and itself factorises with unique factors (as determined by ground truth singular values of $$D$$, $$s_i$$), then the ELBO is maximised when independent components of the model distributions align with those of the data and so p(x) is **disentangled** as a product of *independent components* aligned with each latent dimension.

This is not necessarily surprising in the linear case, since we know from the outset that the push-forward distribution is Gaussian and so factorises as a product of univariate Gaussians. However, we did not use that knowledge or rely on the linearity of $$d$$ at any stage.

<!-- From $$J_z =\tfrac{d x}{d z}=US, \tfrac{d x}{d u} = U$$, we have $$\tfrac{d u}{d z} =S$$, so $$\tfrac{\partial u_i}{\partial z_j} = \{s_i$$ if $$i= j$$, otherwise $$0\}$$ and each $$u_i$$ depends on a distinct independent r.v. so are *independent*.
2. restricting $$D$$ to $$z_i\in \mathcal{Z}^{(i)}$$ and so $$u_i\in\mathcal{M_D^{(i)}}$$, $$p(u_i)= s_i^{-1}p(z_i)$$ (defined over $$\mathcal{M_D^{(i)}}$$)
 -->

---

#### Non-linear Case with Diagonal Jacobian

<p align="center">
  <img src="/assets/disentanglement/non_linear.png" 
        alt="non_linear2" 
        width="420" 
        height="200" 
        style="display: block; margin: 0 auto"  />
  <small>Illustration of non-linear decoder $d:\mathcal{X}\to\mathcal{Z}$. The latent Gaussian prior is mapped to a push-forward distribution in the data space over a non-linear manifold. </small>
</p>


We now take an analogous approach for a general VAE ($$x=d(z)$$, $$d\in\mathcal{C}^2$$) with column-orthogonal decoder Jacobian. Note that the Jacobian and its factors, $$J_z=U_zS_zV_z^\top$$, may vary with $$z$$, however column-orthogonality implies $$V_z=I,\ \forall z\in \mathcal{Z}$$ and $$U_z$$, $$S_z$$ are continuous w.r.t. $$z$$ (since $$d\in\mathcal{C}^2$$). 

As in the linear case, for a point $$z^*\in \mathcal{Z}$$, we define lines $$\mathcal{Z^{(i)}}\subset\mathcal{Z}$$ passing through $$z^*$$ parallel to the standard basis (blue-dashed, axis-aligned), and their images under $$d$$, $$\mathcal{M}_d^{(i)}\subset\mathcal{M_d}$$, which are potentially curved sub-manifolds following (local) left singular vectors of $$J_{z^*}$$ (red-dashed lines).

We again consider $$x=d(z)$$ in the local $$U$$-basis (defined by columns of $$U_z$$) as $$x^{(U)}=U_z^\top x$$ and again have $$\tfrac{\partial x^{(U)}_i}{\partial z_j} =\{s_i \text{ if }i=j; 0 \text{ o/w}\}$$.

We claim that, as before, independent dimensions $$z_i\in\mathcal{Z}$$ flow through the decoder to become independent components $$x^{(U)}_i$$ since:
1. $$\{x^{(U)}_i\}_i$$ are *independent* (by consideration of partial derivatives $$\tfrac{\partial x^{(U)}_i}{\partial z_j}$$);
2. the push-forward of $$d$$ restricted to $$\mathcal{Z^{(i)}}$$ has density $$p(x^{(U)}_i) = s_i^{-1}p(z_i)$$ over $$\mathcal{M}_d^{(i)}$$; and
3. the full push-forward satisfies $$p(d(z)) = \mid J_z\mid ^{-1}p(z) = \prod_i s_i^{-1}p(z_i) = \prod _ip(x^{(U)}_i)$$.

Thus, following the same argument as in the linear case, we see that the distribution over the decoder manifold factorises as a product of independent univariate push-forward distributions ($$p(x^{(U)}_i)$$), each corresponding to a distinct latent dimension $$z_i$$. If the true data distribution follows this generative process and so factorises, then the ELBO is maximised when independent components under the model fit to those of the data, i.e. when **$$p(x)$$ is **disentangled** as a product of *independent components* aligned with each latent dimension**. (Identifiability is subject to uniqueness of independent factor distributions, analogous to the linear case).

> **Key insight**: the above results hinge on the SVD of the Jacobian $$J_z = U_zS_zV_z^\top$$. By differentiability of $$d$$, the bases defined by columns of $$U_z$$ and $$V_z$$ (in $$\mathcal{X}$$ and $$\mathcal{Z}$$ resp.) are continuous and basis vectors form continuous sub-manifolds in each domain. Traversing a submanifold in one domain corresponds to traversing a corresponding submanifold in the other. The diagonal matrix $$S_z$$ defines the Jacobian of the mapping between them (i.e. between $$x$$ considered in the $$U$$-basis and $$z$$ considered in the $$V$$ basis) and so separably maps probability density from each submanifold in $$\mathcal{Z}$$ to its counterpart in $$\mathcal{X}$$. Hence independent components in $$\mathcal{Z}$$ map to independent components in $$\mathcal{X}$$.

**Notes**:
* While we assumed $$d\in\mathcal{C^2}$$ above, the result holds for continuous $d$ that are differentiable almost everywhere, e.g. ReLU networks.
* We highly recommend reading [the full paper][paper] for further details, such as:
  * consideration of whether orthogonality is strictly _necessary_ for disentanglement (the argument above shows it is _sufficient_)
  * consideration of model _identifiability_, i.e. up to what symmetries a VAE can identify ground truth generative factors
  * the role of $\beta$ in a [$\beta$-VAE][betaVAE]. In particular, we show that
     * $$\beta$$ corresponds to Var$$_\theta[x\mid z]$$ when $$p_\theta(x\mid z)$$ is of exponential family form (generalising $\sigma^2$ of a Gaussian-VAE).
     * $\beta$ acts a "glue" determining how close data points need to be (in Euclidean norm) for the model to treat them as "similar", i.e. for their representations to merge.

<!-- 
**Interpreting β in β-VAEs**

β-VAEs introduce a hyperparameter β that scales the KL divergence term in the ELBO. Empirical studies have shown that increasing β enhances disentanglement, often at the cost of reconstruction quality. The sources offer a novel interpretation of β, viewing it as a factor scaling the variance of the likelihood distribution.

Increasing β corresponds to increasing the variance of the likelihood, essentially making the model more uncertain about its reconstructions. This increased uncertainty leads to greater overlap between the posterior distributions of nearby data points. As the ELBO encourages Jacobian orthogonality in expectation over the posterior distributions, larger overlap implies that orthogonality constraints apply over a broader region in the latent space, promoting stronger disentanglement.

Conversely, decreasing β reduces the likelihood variance, mitigating the issue of "posterior collapse". This phenomenon occurs when the likelihood becomes overly expressive, directly modeling the data distribution and rendering the latent variables redundant. By reducing the likelihood variance, β < 1 constrains the model's flexibility, preventing it from learning a trivial solution. -->

---
---

### Final remarks

We hope this post provides intuitive insight into how a seemingly innocuous design choice, motivated by computational efficiency, leads to disentanglement; how understanding may transfer surprisingly usefully  from linear to non-linear models; and, perhaps most fundamentally, how statistically independent components of the data form "seams" running through the data manifold, which a VAE tries to "unpick".

This work throws up many interesting questions, e.g.:
* if the VAE objective aims to disentangle, why is disentanglement not observed more reliably, e.g. as observed by [Locatello et al. (2019)](https://arxiv.org/pdf/1811.12359)?
* how does this finding fit with VAE extensions that enhance entanglement, such as [FactorVAE](https://arxiv.org/abs/1802.05983) and [TC-VAE](https://arxiv.org/abs/1802.04942)?
* can this be used to automatically identify independent factors in the latent space of other models, e.g. GANs?
* given this result is for continuous data domains, how does it translate to discrete data such as text?
* how does this insight translate to other modelling paradigms, such as supervised, semi-supervised and self-supervised learning?

If any of these or related questions are of interest and you would like to collaborate, please free to get in touch (by email, twitter or blue-sky).

Lastly, we welcome any constructive feedback or discussion on this post on blue-sky (@carl-allen.bsky.social - I will pin a link in my profile).

Thanks for reading!

[paper]: https://arxiv.org/pdf/2410.22559
[VAE]: https://arxiv.org/pdf/1312.6114v11
[betaVAE]: https://openreview.net/forum?id=Sy2fzU9gl
[PPCA]: https://academic.oup.com/jrsssb/article-abstract/61/3/611/7083217

[^rolinek]: [Variational Autoencoders Pursue PCA Directions (by Accident); Rolinek et al. (2019)](https://arxiv.org/pdf/1812.06775)
[^kumarpoole]: [On Implicit Regularization in β-VAEs; Kumar \& Poole (2020)](https://arxiv.org/pdf/2002.00041)
[^locatello]: [Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations; Locatello et al. (2019)](https://arxiv.org/pdf/1811.12359)
[^khemakem]: [Variational Autoencoders and Nonlinear ICA: A Unifying Framework; Khemakhem et al. (2020)](https://proceedings.mlr.press/v108/khemakhem20a/khemakhem20a.pdf)
